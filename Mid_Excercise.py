# -*- coding: utf-8 -*-
"""RL_Mid-2023.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M7nlnze98BEmraidyGwHAZ7jgjHXRN8i

# Mid-semester assignment - reinforcements learning 
---
<br>

## Name and ID:
Student 1: Jonathan Erell, 207044447
<br>
Student 2: Etay Todress, 316517689

## Installations and Imports
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !sudo apt-get update
# !sudo apt-get install -y xvfb ffmpeg freeglut3-dev
# !pip install 'imageio==2.4.0'
# !pip install gym
# !pip install pygame
# !apt-get install python-opengl -y
# !apt install xvfb -y
# !pip install pyvirtualdisplay
# !pip install piglet
# !pip install -U --no-cache-dir gdown --pre
# !gdown --id 1FeuIx5OVLmfCx0dxxwU-7Xn8gpPc-53D
# !unzip /content/maze_mid.zip

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
from collections import defaultdict
import seaborn as sns
import gym
import pprint
import random
from gym import logger as gymlogger
from gym.utils import seeding
from gym import error, spaces, utils
gymlogger.set_level(40) # error only
import glob
import pandas as pd
import os, sys 
import io
import base64
import os
import random
import matplotlib.pyplot as plt
# %matplotlib inline
import math
import glob
from pyvirtualdisplay import Display
from IPython.display import HTML
from IPython import display as ipythondisplay
import pygame
from maze_mid import *
from maze_mid.cust_maze import MazeEnvCast5x5, MazeEnvCast15x15, MazeEnvCast25x25
import pyvirtualdisplay
import imageio
import IPython
import time

"""##**Ex. 1 - MDP Implementation, 5x5 maze**

### MDP Class
"""

class MDP_agent_trainer:
  def __init__(self, env, p=0.9, gamma=1, init_V_value=0,
               init_reward='default', init_final_reward=1, delta=0.001):
    
    self.p_stochastic, self.env, self.delta = p, env, delta
    self.terminal_state, self.gamma = None, gamma
    self.rows, self.cols, self.rewards, self.actions, self.nTa = None, None, None, None, None
    self.init_V_value, self.init_reward, self.init_final_reward = init_V_value, init_reward, init_final_reward
    self.episodes_to_V_tracker, self.policy_tracker, self.middle_policy = {}, {}, None
    self.P, self.pi, self.states, self.V = self.init_transition_model()

    self.model_converges = self.run_MDP()

  def init_P(self):
    P = {}
    for i in range(self.rows):
        for j in range(self.cols):
                P.update({(i,j): {action : [] for action in self.actions}})

    for state in P.keys():
        a_actions = self.get_available_actions((state[1], state[0]));
        next_states = [];
        
        for chosen_action in self.actions:
            for actual_action in self.actions:
                next_state = tuple(np.add(state, self.nTa[actual_action][2])) if (actual_action in a_actions) else state;
                if(actual_action == chosen_action):
                    P[state][chosen_action].append([self.p_stochastic, next_state, self.rewards[next_state], 1 if state == self.terminal_state else 0])
                else:
                    P[state][chosen_action].append([(1 - self.p_stochastic) / 3, next_state, self.rewards[next_state], 1 if state == self.terminal_state else 0])
    return P

  def init_transition_model(self):
    self.nTa = {
        0:("UP","N", (-1, 0)),
        1:("DOWN","S", (1, 0)),
        2:("RIGHT","E", (0, 1)),
        3:("LEFT","W",(0, -1))
        }
    

    number_of_states = np.prod((self.env.observation_space.high - self.env.observation_space.low) + 1 )
    self.terminal_state = (self.env.observation_space.high[0], self.env.observation_space.high[1])
    self.actions = np.arange(self.env.action_space.n)

    self.rows = self.env.observation_space.high[0] + 1
    self.cols = self.env.observation_space.high[1] + 1
    maze = self.env.unwrapped.maze_view.maze.maze_cells

    if self.init_reward == 'default':
      self.rewards = np.ones_like(maze, dtype = float) * -(0.1/number_of_states)
      self.rewards[self.terminal_state] = 1

    else:
      self.rewards = np.ones_like(maze, dtype = float) * self.init_reward
      self.rewards[self.terminal_state] = self.init_final_reward
    

    P = self.init_P()
    pi = {state:np.ones_like(self.actions) / len(self.actions) for state in P.keys()}
    states = list(P.keys())
    V = {(i, j): self.init_V_value for i in range(self.rows) for j in range(self.cols)}

    return P, pi, states, V

  def get_action_from_pi_state(self, pi, state):
    """ 
        this function receives pi and state, and for such probabilities
        returns the action which is the most likely. if there's a tie, it will generate
        a random choice of action.
    """
    probabilities = pi[state]
    indices_of_max_prob = list(np.where(probabilities == np.max(probabilities))[0])
    selected_action = random.randrange(0, len(indices_of_max_prob))
    return indices_of_max_prob[selected_action]

  def get_available_actions(self, state):
    """
    this function receives a state, and all possible actions, 
    and returns a list of possible actions for the given state.
    *** the legit check somehow checks index of state as (col, row), which is the 
    opposite of the state ***
    """
    available_actions = []
    for action in self.actions:
        action_letter = self.nTa[action][1];
        
        legit = self.env.unwrapped.maze_view.maze.is_open(state, action_letter)
        if(legit): available_actions.append(action)
    
    return available_actions

  def get_optional_states(self):
    optional_states_dict = {state: {} for state in self.states} 
    available_actions = {state: self.get_available_actions((state[1], state[0])) for state in self.states}
    for state in self.states:
      for action in available_actions[state]:
        new_state = np.add(state, self.nTa[action][2])
        optional_states_dict[state][action] = tuple(new_state)

    return optional_states_dict

  def calc_states_new_value(self, state, temp_V):
    new_state_value = 0
    
    projected_action = self.get_action_from_pi_state(self.pi, state)
    for optional_state in self.P[state][projected_action]:
      transition_probability, next_state, reward, done = optional_state
      new_state_value += transition_probability * (self.rewards[next_state] + self.gamma * self.V[next_state]) 

    return new_state_value

  
  def policy_evaluation(self):
    iterations = 0
    delta = 100
    while self.delta < delta and iterations < 100:
      old_V = self.V.copy()
      for state in self.states:
        v = old_V[state]
        self.V[state] = self.calc_states_new_value(state=state, temp_V=old_V)
        delta = max(delta, abs(v - self.rewards[state]))

      iterations += 1

  
  def calc_best_action_by_pi(self, state):
    """ 
      this is the policy improvement step, we will calculate for each step,
      in which direction we will receive highest score, and return such index
      which represents the action, and the values of all actions for pi 
    """

    best_actions = []
    for action in self.actions:
      temp_state_value = 0
      for optional_state in self.P[state][action]:
        transition_probability, next_state, reward, done = optional_state
        temp_state_value += transition_probability * (self.rewards[next_state] + self.gamma * self.V[next_state]) 
      
      best_actions.append(temp_state_value)
  
    ''' we will normalize the best_actions array, to get prob for each '''
    actions_probs = [action_prob / sum(best_actions) for action_prob in best_actions]
    return np.argmax(np.array(actions_probs)), actions_probs

  def policy_improvement(self):
    is_stable = True
    for state in self.states:
      temp_old_action = np.argmax(self.pi[state])
      chosen_action, self.pi[state] = self.calc_best_action_by_pi(state)
      if temp_old_action != chosen_action:
        is_stable = False

    return is_stable

  def run_MDP(self):
    self.episodes_to_V_tracker[0] = self.V.copy()
    is_stable = False
    episode = 0

    while not is_stable:
      self.policy_tracker[episode] = self.pi
      self.policy_evaluation()
      self.episodes_to_V_tracker[episode] = self.V.copy()
      is_stable = self.policy_improvement()
      episode += 1

      if episode > 20:
        return False

    self.middle_policy = self.policy_tracker[episode // 2]
    return True

  def measure_model_policy(self, measure_rewards=True, measure_steps=True):
    rewards_sum, steps_sum = 0, 0

    for _ in range(10):
      done = False
      state = tuple(self.env.reset())
      observation = (0, 0)
      iterations = 0
      while not done:

        if random.random() < 0.9:
          action_num = self.calc_best_action_by_pi((observation[1], observation[0]))[0]
        else:
          action_num = random.randint(0, 3)
          while action_num == self.calc_best_action_by_pi((observation[1], observation[0]))[0]:
            action_num = random.randint(0, 3)

        action_letter = self.nTa[action_num][1]
        observation, reward, done, info = env.step(action_letter)
        
        rewards_sum += reward
        steps_sum += 1
        iterations += 1
        if iterations > 1000:
          return (None, None)
    
    return (round(rewards_sum / 10, 4), round(steps_sum / 10, 4))

  def plot_learnt_path(self):
    fig, axs = plt.subplots(1, len(self.episodes_to_V_tracker.items()), figsize=(20, 3))
    set_range = lambda new_val: (new_val - min_val) / (max_val - min_val)

    for i, (episode, mat) in enumerate(self.episodes_to_V_tracker.items()):
      max_val = max([val for val in mat.values()])
      min_val = min([val for val in mat.values()])
      maze_array = np.zeros((5, 5))
      for row in range(5):
        for col in range(5):
          maze_array[col, row] = set_range(mat[(col, row)])

      axs[i].set_title(f"After {episode} episodes")
      sns.heatmap(maze_array, linewidth=3, ax=axs[i])

    fig.subplots_adjust(wspace=0.001)
    plt.show()

"""### Experiments"""

def generate_model_score(env, hyperparameters):
  final_models = {}
  final_df = pd.DataFrame(columns=['gamma', 'init_V_value', 'init_rewards', 'init_final_reward', 'delta', 'AVG_rewards', 'AVG_steps', 'Converge'])
  for hyperparams in hyperparameters:
    gamma, init_V_value, init_reward, init_final_reward, delta = hyperparams
    print(f"gamma: {gamma}, init_V_value: {init_V_value}, init_reward: {init_reward}, init_final_reward: {init_final_reward}, delta: {delta}")
    sys.stdout.flush()

    model = MDP_agent_trainer(env, gamma=gamma, init_V_value=init_V_value, init_reward=init_reward,
                              init_final_reward=init_final_reward, delta=delta)
    
    final_models[(gamma, init_V_value, init_reward, init_final_reward, delta)] = model

    reward, steps = model.measure_model_policy() if model.model_converges else (None, None)
    final_df.loc[len(final_df)] = [gamma, init_V_value, init_reward, init_final_reward, delta, reward, steps, model.model_converges]

  return final_models, final_df


gammas = [0.8, 0.9, 1]
init_V_values= [-1, 0, 1]
init_rewards = ['default', -0.01, -0.001, 0]
init_final_rewards = [1, 5, 100]
deltas = [1e-2, 1e-3, 1e-4]

combinations = []
for gamma in gammas:
  for init_V_value in init_V_values:
    for init_reward in init_rewards:
      for init_final_reward in init_final_rewards:
        for delta in deltas:
          combinations.append((gamma, init_V_value, init_reward, init_final_reward, delta))

display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()
env = MazeEnvCast5x5()
final_models_MDP, final_df_MDP = generate_model_score(env, combinations)

sorted_hp_df_MDP = final_df_MDP.sort_values(['AVG_rewards', 'AVG_steps'],
              ascending = [False, True])
sorted_hp_df_MDP.head(5)

"""### MDP videos"""

def embed_mp4(filename):
  """Embeds an mp4 file in the notebook."""
  video = open(filename,'rb').read()
  b64 = base64.b64encode(video)
  tag = '''
  <video width="640" height="480" controls>
    <source src="data:video/mp4;base64,{0}" type="video/mp4">
  Your browser does not support the video tag.
  </video>'''.format(b64.decode())

  return IPython.display.HTML(tag)

def generate_video_MDP(env, model, filename, fps=5):
  env.reset()
  env.render()
  if filename == 'half_model':
    model.policy = model.middle_policy

  observation = (0, 0)
  done = False
  iter = 0
  start_time = time.time()
  filename += '.mp4'
  with imageio.get_writer(filename, fps=fps) as video:
    video.append_data(env.render(mode='rgb_array'))
    while not done:
      if random.random() < 0.9:
        action_num = model.calc_best_action_by_pi((observation[1], observation[0]))[0]
      else:
        action_num = random.randint(0, 3)
        while action_num == model.calc_best_action_by_pi((observation[1], observation[0]))[0]:
          action_num = random.randint(0, 3)

      action_letter = model.nTa[action_num][1]
      observation, reward, done, info = env.step(action_letter)
      video.append_data(env.render(mode='rgb_array'))

display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()
env = MazeEnvCast5x5()

best_params = list(sorted_hp_df_MDP.iloc[-5].values[:5])
gamma, init_V_value, init_reward, init_final_reward, delta = best_params

best_model = MDP_agent_trainer(env, gamma=0.8, init_V_value=0, init_reward=0,
                              init_final_reward=5, delta=0.001)

generate_video_MDP(env=env, model=best_model, filename='full_model_MDP')
generate_video_MDP(env=env, model=best_model, filename='half_model_MDP')

"""#### Fully MDP 5x5 maze"""

embed_mp4('full_model_MDP.mp4')

"""#### Half Trained MDP 5x5 maze"""

embed_mp4('half_model_MDP.mp4')

"""##**Ex. 2 - MC, SARSA and Q-Learning Implementation, 15x15 maze**

#### Displaying Functions
"""

def embed_mp4(filename):
  """Embeds an mp4 file in the notebook."""
  video = open(filename,'rb').read()
  b64 = base64.b64encode(video)
  tag = '''
  <video width="640" height="480" controls>
    <source src="data:video/mp4;base64,{0}" type="video/mp4">
  Your browser does not support the video tag.
  </video>'''.format(b64.decode())

  return IPython.display.HTML(tag)

def generate_video(env, model, filename, fps=5):
  env.reset()
  observation = (0, 0)
  done = False  
  if 'half_model' in filename:
    model.policy = model.middle_model

  filename += '.mp4'
  with imageio.get_writer(filename, fps=fps) as video:
    video.append_data(env.render(mode='rgb_array'))
    while not done:        
      if random.random() < 0.9:
        action_num = model.policy[observation]
      else:
        action_num = random.randint(0, 3)
        while action_num == model.policy[observation]:
          action_num = random.randint(0, 3)

      action_letter = model.nTa[action_num][1]
      observation, reward, done, info = env.step(action_letter)
      observation = tuple(observation)
      video.append_data(env.render(mode='rgb_array'))

def plot_convergence(df):
    plt.figure(figsize=(10, 5))

    plt.subplot(1, 2, 1)
    sns.lineplot(data=df, x=df.index, y='steps')
    plt.title("Steps as function of episodes")

    plt.subplot(1, 2, 2)
    sns.lineplot(data=df, x=df.index, y='rewards')
    plt.title("Rewards as function of episodes")

    plt.show()

def plot_learnt_path(model):
  fig, axs = plt.subplots(1, len(model.episodes_to_starting_states_tracker.items()), figsize=(20, 3))
  set_range = lambda new_val: (new_val - min_val) / (max_val - min_val)

  for i, (episode, mat) in enumerate(model.episodes_to_starting_states_tracker.items()):
    max_val = max([val for val in mat.values()])
    min_val = min([val for val in mat.values()])
    maze_array = np.zeros((15, 15))
    for row in range(15):
      for col in range(15):
        maze_array[row, col] = set_range(mat[(col, row)])

    axs[i].set_title(f"After {episode} episodes")
    sns.heatmap(maze_array, linewidth=3, ax=axs[i])

  fig.subplots_adjust(wspace=0.001)
  plt.show()

def measure_model_policy(model):
  rewards_sum, steps_sum = 0, 0
  for _ in range(100):
    done = False
    state = tuple(model.env.reset())
    c = 0

    while not done:
      if c > 1000:
        return None, None
      if state not in model.policy:
        action = model.env.action_space.sample()
      else:
        if random.random() < model.p:
          action = model.policy[state]
        else:
          action = random.randint(0, 3)
          while action == model.policy[state]:
            action = random.randint(0, 3)

      next_state, reward, done, info = model.env.step(model.nTa[action][1])
      rewards_sum += reward
      steps_sum += 1
      state = tuple(next_state)
  
  return round(rewards_sum / 100, 4), round(steps_sum / 100, 4)

"""###Q - Learning Class -"""

class QL_agent_trainer:
  def __init__(self, env, n_episodes=500, gamma=0.99, alpha=0.1, epsilon=0.1,
               p_stochastic=0.9, use_hot_start=True, use_greedy_epsilon=True,
               init_maze_reward='default', init_final_state_reward='default'):
    
    self.p, self.gamma, self.alpha, self.n_episodes = p_stochastic, gamma, alpha, n_episodes
    self.env, self.starting_states, self.df = env, {}, pd.DataFrame(columns=['steps', 'rewards'])
    self.episodes_to_starting_states_tracker, self.init_maze_reward = {}, init_maze_reward
    self.init_final_state_reward = init_final_state_reward
    self.use_hot_start, self.use_greedy_epsilon = use_hot_start, use_greedy_epsilon
    self.nTa = {
            0:("UP","N", (0, -1)),
            1:("DOWN","S", (0, 1)),
            2:("RIGHT","E", (1, 0)),
            3:("LEFT","W",(-1, 0))
          }

    self.epsilon, self.nA, self.actions, self.Q, self.policy = epsilon, None, None, None, None
    self.middle_model = None
    self.run_QL()


  def update_Q(self, state, next_state, reward, action):
    """
      updating Q using the formula:
      Q(S,A) += alpha[R + lr * Q_argmax_a(S',a) - Q(S,A)]
    """
    predict = self.Q[state][action]
    target = reward + self.gamma * np.max(self.Q[next_state])
    self.Q[state][action] += self.alpha * (target - predict)


  def get_starting_state(self):
      """  
        this function is used the generate the starting state for an episode.
        this is used as an exploration, where we start fromm the least visited state
      """
      sorted_list_of_starting_state = sorted(self.starting_states.items(), key=lambda item: item[1])
      first_elem, sec_elem = sorted_list_of_starting_state[0][0], sorted_list_of_starting_state[1][0]
      return first_elem if first_elem != (14, 14) else sec_elem

  def get_specific_reward(self, state, default_reward):
    """ this function is used to try custon init values for rewards """
    if state == (14, 14):
      return default_reward if self.init_final_state_reward == 'default' else self.init_final_state_reward
    else:
      return default_reward if self.init_maze_reward == 'default' else self.init_maze_reward
  
  def generate_episode_from_Q(self, i_episode):
      """ generates an episode from following the epsilon-greedy policy """
      episode = []
      env.reset()

      ''' exploration using random start state  '''
      if self.use_hot_start:
        env.state = self.get_starting_state()

      state = env.state

      if not isinstance(state, tuple):
          state = tuple(state)

      self.starting_states[state] += 1
      num_of_steps, tot_reward_of_episode = 0, 0

      while True:
        if not isinstance(state, tuple):
          state = tuple(state)

        if state not in self.Q:
          action = self.env.action_space.sample()
        else:
          action = self.generate_action_from_state(state)

        next_state, reward, done, info = env.step(self.nTa[action][1])
        next_state = tuple(next_state)
        reward = self.get_specific_reward(next_state, reward)

        ''' keeping track of visited states for later uses of starting state '''
        
        self.starting_states[state] += 1
        self.starting_states[next_state] += 1

        self.update_Q(state, next_state, reward, action)

        state = next_state
        num_of_steps += 1
        tot_reward_of_episode += reward

        if done:
          print("\rEpisode {}/{}, {} iterations.".format(i_episode, self.n_episodes, num_of_steps), end="")
          sys.stdout.flush()
          self.df.loc[len(self.df)] = num_of_steps, tot_reward_of_episode
          break


  def generate_action_from_state(self, state):
    if self.use_greedy_epsilon:
      policy_recommended_action = self.epsilon_greedy_action(self.Q[state])
    
    else:
      policy_recommended_action = np.argmax(self.Q[state])

    ''' using stochastic p to generate an action '''
    probs = np.where(self.actions == policy_recommended_action, self.p, (1 - self.p) / 3)
    action = np.random.choice(self.actions, p=probs)
    
    return action


  def epsilon_greedy_action(self, Qs):
      """ 
      obtains the action probabilities corresponding to epsilon-greedy policy.
      we will use random tie breaker in case of same probabilities
      """
      
      indices_of_max_prob = list(np.where(Qs == np.max(Qs))[0])
      selected_action = random.randrange(0, len(indices_of_max_prob))
      best_a = indices_of_max_prob[selected_action]

      greedy_policy = np.ones(self.nA) * self.epsilon / self.nA
      greedy_policy[best_a] = 1 - self.epsilon + (self.epsilon / self.nA)

      if random.random() < self.epsilon:
        non_best_actions = [i for i in self.actions if i != best_a]
        selected_non_best_action = random.randrange(0, len(non_best_actions))
        return non_best_actions[selected_non_best_action]

      else:
        return best_a

  def run_QL(self):
    self.nA = env.action_space.n
    self.actions = np.array([i for i in range(env.action_space.n)])
    rows = self.env.observation_space.high[0] + 1
    cols = self.env.observation_space.high[1] + 1
    self.starting_states = {(row, col): 0 for row in range(rows) for col in range(cols)}
    self.Q = defaultdict(lambda: np.zeros(self.nA))
    
    for i_episode in range(1, self.n_episodes + 1):       
        self.generate_episode_from_Q(i_episode)

        if i_episode == self.n_episodes // 2:
          self.middle_model = dict((k,np.argmax(v)) for k, v in self.Q.items())
        
        if i_episode in [self.n_episodes, (self.n_episodes * 3) // 4, self.n_episodes // 4, self.n_episodes // 2]:
          self.episodes_to_starting_states_tracker[i_episode] = self.starting_states.copy()

    self.policy = dict((k,np.argmax(v)) for k, v in self.Q.items())

"""####Experiments"""

def generate_model_score(env, hyperparameters):
  final_models = {}
  final_df = pd.DataFrame(columns=['gamma', 'alpha', 'epsilon', 'Greedy Epsilon', 'Hot Start', 'Init Maze Reward', 'Final State Reward', 'AVG_rewards', 'AVG_steps'])
  for hyperparams in hyperparameters:
    gamma, alpha, epsilon, use_greedy_epsilon, use_hot_start, init_maze_reward, init_final_state_reward = hyperparams
    print(f"gamma: {gamma}, alpha: {alpha}, epsilon: {epsilon}, Greedy Epsilon: {use_greedy_epsilon}, Hot Start: {use_hot_start}, Init Maze Reward: {init_maze_reward}, final state reward: {init_final_state_reward}")
    sys.stdout.flush()

    model = QL_agent_trainer(env=env, gamma=gamma, alpha=alpha, epsilon=epsilon, n_episodes=n_episodes,
                             use_greedy_epsilon=use_greedy_epsilon, use_hot_start=use_hot_start,
                             init_maze_reward=init_maze_reward, init_final_state_reward=init_final_state_reward)
    
    final_models[(gamma, alpha, epsilon, use_greedy_epsilon, use_hot_start, init_maze_reward, init_final_state_reward)] = model

    reward, steps = measure_model_policy(model)
    final_df.loc[len(final_df)] = [gamma, alpha, epsilon, use_greedy_epsilon, use_hot_start, init_maze_reward, init_final_state_reward, reward, steps]

  return final_models, final_df

gammas=[0.8, 0.9]
alphas = [0.1]
epsilons = [0.05, 0.1]
init_maze_rewards = ['default', -0.0001]
init_final_state_rewards = ['default', 5]
n_episodes=500
use_greedy_epsilons = [True, False]
use_hot_starts = [False]

combinations = []
for gamma in gammas:
  for alpha in alphas:
    for epsilon in epsilons:
      for use_greedy_epsilon in use_greedy_epsilons:
        for init_maze_reward in init_maze_rewards:
          for init_final_state_reward in init_final_state_rewards:
            for use_hot_start in use_hot_starts:
              combinations.append((gamma, alpha, epsilon, use_greedy_epsilon,
                                   use_hot_start, init_maze_reward, init_final_state_reward))
              

display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()
env = MazeEnvCast15x15()
final_models_QL, final_df_QL = generate_model_score(env, combinations)

sorted_hp_df_QL = final_df_QL.sort_values(['AVG_rewards', 'AVG_steps'],
                                            ascending = [False, True])
best_params = list(sorted_hp_df_QL.iloc[0].values[:7])
best_model = final_models_QL[tuple(best_params)]

"""####Results' Graphs

- The first graph shows the number of times each state has been visited. As we can see, as the episodes progressing, the path is clearer.

- The second graph shows the total rewards, and the number of steps at each episode. As we can see, the rewards increase and the steps decrease.

- The third plot is a dataframe which shows the best hyperparameters.
it is sorted by the number of steps (average of 100 runs), and total rewards (average of 100 runs)



"""

plot_learnt_path(best_model)

plot_convergence(best_model.df)

sorted_hp_df_QL.head(5)

"""####Q - Learning Videos"""

best_model = QL_agent_trainer(env=env, gamma=0.9, alpha=0.1, epsilon=0.1, n_episodes=500,
                             use_greedy_epsilon=True, use_hot_start=False,
                             init_maze_reward=-0.0001, init_final_state_reward='default')

display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()
env = MazeEnvCast15x15()
generate_video(env=env, model=best_model, filename='full_model_QL')
generate_video(env=env, model=best_model, filename='half_model_QL')

"""#####Fully Q - Learning 15x15 maze"""

embed_mp4('full_model_QL.mp4')

"""#####Half Trained Q - Learning 15x15 maze"""

embed_mp4('half_model_QL.mp4')

"""### SARSA Class - """

class SARSA_agent_trainer:
  def __init__(self, env, n_episodes=500, gamma=1, alpha=0.1, epsilon=0.1,
               p_stochastic=0.9, use_hot_start=False, use_greedy_epsilon=True,
               init_final_state_reward='default', init_maze_reward='default'):
    
    self.p, self.gamma, self.alpha, self.n_episodes = p_stochastic, gamma, alpha, n_episodes
    self.env, self.starting_states, self.df = env, {}, pd.DataFrame(columns=['steps', 'rewards'])
    self.episodes_to_starting_states_tracker, self.init_maze_reward = {}, init_maze_reward
    self.init_final_state_reward = init_final_state_reward
    self.use_hot_start, self.use_greedy_epsilon = use_hot_start, use_greedy_epsilon
    self.nTa = {
            0:("UP","N", (0, -1)),
            1:("DOWN","S", (0, 1)),
            2:("RIGHT","E", (1, 0)),
            3:("LEFT","W",(-1, 0))
          }

    self.epsilon, self.nA, self.actions, self.Q, self.N, self.policy = epsilon, None, None, None, None, None
    self.middle_model = None
    self.run_SARSA()


  def update_Q(self, state, next_state, reward, action, next_action):
    """
      updating Q using the formula:
      Q(S,A) += alpha[R + lr * Q(S',a') - Q(S,A)]
    """
    predict = self.Q[state][action]
    target = reward + self.gamma * self.Q[next_state][next_action]
    self.Q[state][action] += self.alpha * (target - predict)

  def get_starting_state(self):
      """  
        this function is used the generate the starting state for an episode.
        this is used as an exploration, where we start fromm the least visited state
      """
      sorted_list_of_starting_state = sorted(self.starting_states.items(), key=lambda item: item[1])
      first_elem, sec_elem = sorted_list_of_starting_state[0][0], sorted_list_of_starting_state[1][0]
      return first_elem if first_elem != (self.env.observation_space.high[1] + 1, self.env.observation_space.high[0] + 1) else sec_elem

  def get_specific_reward(self, state, default_reward):
    """ this function is used to try custon init values for rewards """
    if state == (14, 14):
      return default_reward if self.init_final_state_reward == 'default' else self.init_final_state_reward
    else:
      return default_reward if self.init_maze_reward == 'default' else self.init_maze_reward
  
  def generate_episode_from_Q(self, i_episode):
      """ generates an episode from following the epsilon-greedy policy """
      episode = []
      self.env.reset()

      ''' exploration using hot start state  '''
      if self.use_hot_start:
        self.env.state = self.get_starting_state()
      
      if not isinstance(self.env.state, tuple):
          self.env.state = tuple(self.env.state)

      state = self.env.state
      self.starting_states[self.env.state] += 1

      if state not in self.Q:
          action = self.env.action_space.sample()
      else:
        action = self.generate_action_from_state(state)

      num_of_steps, tot_reward_of_episode = 0, 0
      while True:
        if not isinstance(state, tuple):
          state = tuple(state)

        next_state, reward, done, info = self.env.step(self.nTa[action][1])
        next_state = tuple(next_state)
        reward = self.get_specific_reward(next_state, reward)
        next_action = self.generate_action_from_state(next_state)

        ''' keeping track of visited states for later uses of starting state '''
        
        self.starting_states[state] += 1
        self.starting_states[next_state] += 1

        self.update_Q(state, next_state, reward, action, next_action)

        state = next_state
        action = next_action
        num_of_steps += 1
        tot_reward_of_episode += reward

        if done:
          print("\rEpisode {}/{}, {} iterations.".format(i_episode, self.n_episodes, num_of_steps), end="")
          sys.stdout.flush()
          self.df.loc[len(self.df)] = num_of_steps, tot_reward_of_episode
          break


  def generate_action_from_state(self, state):
    if self.use_greedy_epsilon:
      policy_recommended_action = self.epsilon_greedy_action(self.Q[state])
    
    else:
      policy_recommended_action = np.argmax(self.Q[state])

    ''' using stochastic p to generate an action '''
    probs = np.where(self.actions == policy_recommended_action, self.p, (1 - self.p) / 3)
    action = np.random.choice(self.actions, p=probs)
    
    return action


  def epsilon_greedy_action(self, Qs):
      """ 
      obtains the action probabilities corresponding to epsilon-greedy policy.
      we will use random tie breaker in case of same probabilities
      """
      
      indices_of_max_prob = list(np.where(Qs == np.max(Qs))[0])
      selected_action = random.randrange(0, len(indices_of_max_prob))
      best_a = indices_of_max_prob[selected_action]

      greedy_policy = np.ones(self.nA) * self.epsilon / self.nA
      greedy_policy[best_a] = 1 - self.epsilon + (self.epsilon / self.nA)

      if random.random() < self.epsilon:
        non_best_actions = [i for i in self.actions if i != best_a]
        selected_non_best_action = random.randrange(0, len(non_best_actions))
        return non_best_actions[selected_non_best_action]

      else:
        return best_a

  def run_SARSA(self):
    self.nA = self.env.action_space.n
    self.actions = np.array([i for i in range(self.env.action_space.n)])
    rows = self.env.observation_space.high[0] + 1
    cols = self.env.observation_space.high[1] + 1
    self.starting_states = {(row, col): 0 for row in range(rows) for col in range(cols)}

    self.Q = defaultdict(lambda: np.zeros(self.nA))
    
    for i_episode in range(1, self.n_episodes + 1):     
        self.generate_episode_from_Q(i_episode)

        if i_episode == self.n_episodes // 2:
          self.middle_model = dict((k,np.argmax(v)) for k, v in self.Q.items())
        
        if i_episode in [self.n_episodes, (self.n_episodes * 3) // 4, self.n_episodes // 4, self.n_episodes // 2]:
          self.episodes_to_starting_states_tracker[i_episode] = self.starting_states.copy()

    self.policy = dict((k,np.argmax(v)) for k, v in self.Q.items())

"""#### Experiments"""

def generate_model_score(hyperparameters):
  final_models = {}
  final_df = pd.DataFrame(columns=['gamma', 'alpha', 'epsilon', 'Greedy Epsilon', 'Hot Start', 'Init Maze Reward', 'Final State Reward', 'AVG_rewards', 'AVG_steps'])
  for hyperparams in hyperparameters:
    gamma, alpha, epsilon, use_greedy_epsilon, use_hot_start, init_maze_reward, init_final_state_reward = hyperparams
    print(f"gamma: {gamma}, alpha: {alpha}, epsilon: {epsilon}, Greedy Epsilon: {use_greedy_epsilon}, Hot Start: {use_hot_start}, Init Maze Reward: {init_maze_reward}, final state reward: {init_final_state_reward}")
    sys.stdout.flush()
    
    display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()
    env = MazeEnvCast15x15()
    model = SARSA_agent_trainer(env=env, gamma=gamma, alpha=alpha, epsilon=epsilon, n_episodes=n_episodes,
                                use_greedy_epsilon=use_greedy_epsilon, use_hot_start=use_hot_start,
                                init_maze_reward=init_maze_reward, init_final_state_reward=init_final_state_reward)
        
    final_models[(gamma, alpha, epsilon, use_greedy_epsilon, use_hot_start, init_maze_reward, init_final_state_reward)] = model

    reward, steps = measure_model_policy(model)
    final_df.loc[len(final_df)] = [gamma, alpha, epsilon, use_greedy_epsilon, use_hot_start, init_maze_reward, init_final_state_reward, reward, steps]

  return final_models, final_df


gammas=[0.8, 0.9]
alphas = [0.1, 0.2]
epsilons = [0.05, 0.1, 0.15]
init_maze_rewards = ['default', -0.0001]
init_final_state_rewards = ['default', 5]
n_episodes=500
use_greedy_epsilons = [True, False]
use_hot_starts = [False]

combinations = []
for gamma in gammas:
  for alpha in alphas:
    for epsilon in epsilons:
      for use_greedy_epsilon in use_greedy_epsilons:
        for init_maze_reward in init_maze_rewards:
          for init_final_state_reward in init_final_state_rewards:
            for use_hot_start in use_hot_starts:
              combinations.append((gamma, alpha, epsilon, use_greedy_epsilon,
                                   use_hot_start, init_maze_reward, init_final_state_reward))

final_models_SARSA, final_df_SARSA = generate_model_score(combinations)

sorted_hp_df_SARSA = final_df_SARSA.sort_values(['AVG_rewards', 'AVG_steps'],
                                                  ascending = [False, True])

best_params = list(sorted_hp_df_SARSA.iloc[0].values[:7])
best_model =  final_models_SARSA[tuple(best_params)]

"""####Results' Graphs
- The first graph shows the number of times each state has been visited. As we can see, as the episodes progressing, the path is clearer.

- The second graph shows the total rewards, and the number of steps at each episode. As we can see, the rewards increase and the steps decrease.

- The third plot is a dataframe which shows the best hyperparameters.
it is sorted by the number of steps (average of 100 runs), and total rewards (average of 100 runs)


"""

plot_learnt_path(best_model)

plot_convergence(best_model.df)

sorted_hp_df_SARSA.head(5)

"""#### SARSA Videos"""

best_model = SARSA_agent_trainer(env=env, gamma=0.8, alpha=0.1, epsilon=0.15, n_episodes=500,
                                use_greedy_epsilon=False, use_hot_start=False,
                                init_maze_reward='default', init_final_state_reward=5)

display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()
env = MazeEnvCast15x15()

generate_video(env=env, model=best_model, filename='full_model_SARSA')
generate_video(env=env, model=best_model, filename='half_model_SARSA')

"""#####  Fully SARSA 15x15 maze"""

embed_mp4('full_model_SARSA.mp4')

"""##### Half Trained SARSA 15x15 maze"""

embed_mp4('half_model_SARSA.mp4')

"""### MC Class - """

class MC_agent_trainer:
  def __init__(self, env, n_episodes=1000, gamma=1, epsilon=0.1, p_stochastic=0.9,
               use_greedy_epsilon=False, use_hot_start=True,
               init_maze_reward='default', init_final_state_reward='default'):
    
    self.p, self.gamma, self.n_episodes = p_stochastic, gamma, n_episodes
    self.env, self.starting_states, self.use_hot_start = env, {}, use_hot_start
    self.init_maze_reward, self.init_final_state_reward = init_maze_reward, init_final_state_reward
    self.use_greedy_epsilon, self.df = use_greedy_epsilon, pd.DataFrame(columns=['steps', 'rewards'])
    self.nTa = {
            0:("UP","N", (0, -1)),
            1:("DOWN","S", (0, 1)),
            2:("RIGHT","E", (1, 0)),
            3:("LEFT","W",(-1, 0))
          }

    self.epsilon, self.nA, self.actions, self.Q, self.N, self.policy = epsilon, None, None, None, None, None
    self.episodes_to_starting_states_tracker, self.middle_model = {}, None
    self.run_GLIE()
  
  def get_starting_state(self):
      """  
        this function is used the generate the starting state for an episode.
        this is used as an exploration, where we start fromm the least visited state
      """
      sorted_list_of_starting_state = sorted(self.starting_states.items(), key=lambda item: item[1])
      first_elem, sec_elem = sorted_list_of_starting_state[0][0], sorted_list_of_starting_state[1][0]
      return first_elem if first_elem != (14, 14) else sec_elem

  def get_specific_reward(self, state, default_reward):
    """ this function is used to try custon init values for rewards """
    if state == (14, 14):
      return default_reward if self.init_final_state_reward == 'default' else self.init_final_state_reward
    else:
      return default_reward if self.init_maze_reward == 'default' else self.init_maze_reward


  def generate_episode_from_Q(self):
      """ generates an episode from following the epsilon-greedy policy """
      episode = []
      state = tuple(self.env.reset())
      
      ''' exploration using random start state  '''
      if self.use_hot_start:
        env.state = self.get_starting_state()

      if not isinstance(env.state, tuple):
        env.state = tuple(env.state)

      state = env.state
      self.starting_states[env.state] += 1
      c = 0
      num_of_steps, tot_reward_of_episode = 0, 0
      while True:
        if len(episode) > 200000:
          return None

        if state not in self.Q:
          action = self.env.action_space.sample()

        else:
          action = self.generate_action_from_state(state)

        next_state, reward, done, info = env.step(self.nTa[action][1])
        reward = self.get_specific_reward(tuple(next_state), reward)
        episode.append((state, action, reward))
        state = tuple(next_state)
        self.starting_states[state] += 1
        num_of_steps += 1
        tot_reward_of_episode += reward

        if done:
          self.df.loc[len(self.df)] = num_of_steps, tot_reward_of_episode
          break
      
      return episode

  def generate_action_from_state(self, state, force_greedy_epsilon=True):
    if self.use_greedy_epsilon and force_greedy_epsilon:
      policy_recommended_action = self.epsilon_soft_action(self.Q[state])
    
    else:
      policy_recommended_action = self.get_probs(self.Q[state])

    ''' using stochastic p to generate an action '''
    probs = np.where(self.actions == policy_recommended_action, self.p, (1 - self.p) / 3)
    action = np.random.choice(self.actions, p=probs)
    
    return action

  def epsilon_soft_action(self, Qs):
      """ 
      obtains the action probabilities corresponding to epsilon-greedy policy.
      we will use random tie breaker in case of same probabilities
      """
      
      indices_of_max_prob = list(np.where(Qs == np.max(Qs))[0])
      selected_action = random.randrange(0, len(indices_of_max_prob))
      best_a = indices_of_max_prob[selected_action]

      greedy_policy = np.ones(self.nA) * self.epsilon / self.nA
      greedy_policy[best_a] = 1 - self.epsilon + (self.epsilon / self.nA)

      if random.random() < self.epsilon:
        non_best_actions = [i for i in self.actions if i != best_a]
        selected_non_best_action = random.randrange(0, len(non_best_actions))
        return non_best_actions[selected_non_best_action]

      else:
        return best_a

  def get_probs(self, Qs):
      """ obtains the action probabilities corresponding to epsilon-greedy policy """

      indices_of_max_prob = list(np.where(Qs == np.max(Qs))[0])
      selected_action = random.randrange(0, len(indices_of_max_prob))
      best_a = indices_of_max_prob[selected_action]
      return best_a


  def update_Q_GLIE(self, episode):
    """ updates the action-value function estimate using the most recent episode """
    states, actions, rewards = zip(*episode)
    discounts = np.array([self.gamma**i for i in range(len(rewards)+1)])
    rewards = np.array(rewards)
    
    for i, state in enumerate(states):
        old_Q = self.Q[state][actions[i]] 
        old_N = self.N[state][actions[i]]
        self.Q[state][actions[i]] += np.divide(np.subtract(np.sum(np.multiply(rewards[i:], discounts[:-(1+i)])), old_Q), old_N + 1)
        self.N[state][actions[i]] += 1

  def run_GLIE(self):
    self.nA = env.action_space.n
    self.actions = np.array([i for i in range(env.action_space.n)])
    rows = self.env.observation_space.high[0] + 1
    cols = self.env.observation_space.high[1] + 1


    self.Q = defaultdict(lambda: np.zeros(self.nA))
    self.N = defaultdict(lambda: np.zeros(self.nA))
    
    self.starting_states = {(row, col): 0 for row in range(rows) for col in range(cols)}
    self.policy = {state:np.ones_like(self.actions) / len(self.actions) for state in self.starting_states.keys()}

    for i_episode in range(1, self.n_episodes + 1):
        if i_episode % 1 == 0:
            print("\rEpisode {}/{}.".format(i_episode, self.n_episodes), end="")
            sys.stdout.flush()
       
        episode = self.generate_episode_from_Q()
        if episode is None:
          continue
          
        self.update_Q_GLIE(episode)

        if i_episode == self.n_episodes // 2:
          self.middle_model = dict((k,np.argmax(v)) for k, v in self.Q.items())

        if i_episode in [self.n_episodes, (self.n_episodes * 3) // 4, self.n_episodes // 4, self.n_episodes // 2]:
            self.episodes_to_starting_states_tracker[i_episode] = self.starting_states.copy()

    self.policy = dict((k,np.argmax(v)) for k, v in self.Q.items())

"""####Experiments"""

def generate_model_score(env, hyperparameters):
  final_models = {}
  final_df = pd.DataFrame(columns=['gamma', 'epsilon', 'Greedy Epsilon', 'Hot Start', 'Init Maze Rewards', 'Init Final Reward', 'AVG_rewards', 'AVG_steps'])
  for hyperparams in hyperparameters:
    gamma, epsilon, use_greedy_epsilon, use_hot_start, init_maze_reward, init_final_state_reward = hyperparams
    print(f"gamma: {gamma}, epsilon: {epsilon}, Greedy Epsilon: {use_greedy_epsilon}, Hot Start: {use_hot_start}, init maze reward: {init_maze_reward}, init final reward: {init_final_state_reward}")
    sys.stdout.flush()

    model = MC_agent_trainer(env=env, gamma=gamma, epsilon=epsilon, n_episodes=n_episodes,
                             use_greedy_epsilon=use_greedy_epsilon, use_hot_start=use_hot_start,
                             init_maze_reward=init_maze_reward, init_final_state_reward=init_final_state_reward)
    
    final_models[(gamma, epsilon, use_greedy_epsilon, use_hot_start, init_maze_reward, init_final_state_reward)] = model

    reward, steps = measure_model_policy(model)
    final_df.loc[len(final_df)] = [gamma, epsilon, use_greedy_epsilon, use_hot_start, init_maze_reward, init_final_state_reward, reward, steps]

  return final_models, final_df

display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()
env = MazeEnvCast15x15()

gammas=[0.9, 0.99]
epsilons = [0.05, 0.1, 0.15]
n_episodes=500
init_maze_rewards = ['default', -0.0001]
init_final_state_rewards = ['default', 5]
use_greedy_epsilons = [False, True]
use_hot_starts = [False]
combinations = []
for gamma in gammas:
  for epsilon in epsilons:
    for use_greedy_epsilon in use_greedy_epsilons:
      for init_maze_reward in init_maze_rewards:
        for init_final_state_reward in init_final_state_rewards:
          for use_hot_start in use_hot_starts:
            combinations.append((gamma, epsilon, use_greedy_epsilon, use_hot_start, init_maze_reward, init_final_state_reward))

display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()
env = MazeEnvCast15x15()
final_models_MC, final_df_MC = generate_model_score(env, combinations)
sorted_hp_df_MC = final_df_MC.sort_values(['AVG_rewards', 'AVG_steps'],
              ascending = [False, True])

best_params = list(sorted_hp_df_MC.iloc[0].values[:6])

best_model = final_models_MC[tuple(best_params)]

"""####Results' Graphs

- The first graph shows the number of times each state has been visited. As we can see, as the episodes progressing, the path is clearer.

- The second graph shows the total rewards, and the number of steps at each episode. As we can see, the rewards increase and the steps decrease.

- The third plot is a dataframe which shows the best hyperparameters.
it is sorted by the number of steps (average of 100 runs), and total rewards (average of 100 runs)



"""

plot_learnt_path(best_model)

plot_convergence(best_model.df)

sorted_hp_df_MC.head(5)

"""#### MC Videos"""

best_model = MC_agent_trainer(env=env, gamma=0.9, epsilon=0.05, n_episodes=500,
                             use_greedy_epsilon=True, use_hot_start=False,
                             init_maze_reward='default', init_final_state_reward=5)

display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()
env = MazeEnvCast15x15()

generate_video(env=env, model=best_model, filename='full_model_MC')
generate_video(env=env, model=best_model, filename='half_model_MC')

"""##### Fully MC 15x15 maze"""

embed_mp4('full_model_MC.mp4')

"""##### Half Trained MC 15x15 maze"""

embed_mp4('half_model_MC.mp4')

"""##**Ex. 3 - 25x25 maze**

###**Custom SARSA - 25x25 maze**
"""

class SARSA_custom_agent_trainer:
  def __init__(self, env, n_episodes=500, gamma=1, alpha=0.1, epsilon=0.1,
               p_stochastic=0.9, use_hot_start=True, use_greedy_epsilon=True,
               positive_reward=None, negative_reward=None,
               positive_state=None, negative_state=None,
               init_maze_reward='default', init_final_state_reward='default'):
    
    self.p, self.gamma, self.alpha, self.n_episodes = p_stochastic, gamma, alpha, n_episodes
    self.env, self.starting_states, self.df = env, {}, pd.DataFrame(columns=['steps', 'rewards'])
    self.episodes_to_starting_states_tracker = {}
    self.positive_state, self.negative_state = positive_state, negative_state
    self.positive_reward, self.negative_reward = positive_reward, negative_reward
    self.init_final_state_reward, self.init_maze_reward = init_final_state_reward, init_maze_reward
    self.use_hot_start, self.use_greedy_epsilon = use_hot_start, use_greedy_epsilon
    self.nTa = {
            0:("UP","N", (0, -1)),
            1:("DOWN","S", (0, 1)),
            2:("RIGHT","E", (1, 0)),
            3:("LEFT","W",(-1, 0))
          }

    self.epsilon, self.nA, self.actions, self.Q, self.N, self.policy = epsilon, None, None, None, None, None
    self.middle_model = None
    self.run_SARSA()


  def update_Q(self, state, next_state, reward, action, next_action):
    """
      updating Q using the formula:
      Q(S,A) += alpha[R + lr * Q(S',a') - Q(S,A)]
    """
    predict = self.Q[state][action]
    target = reward + self.gamma * self.Q[next_state][next_action]
    self.Q[state][action] += self.alpha * (target - predict)


  def get_starting_state(self):
      """  
        this function is used the generate the starting state for an episode.
        this is used as an exploration, where we start fromm the least visited state
      """
      sorted_list_of_starting_state = sorted(self.starting_states.items(), key=lambda item: item[1])
      first_elem, sec_elem = sorted_list_of_starting_state[0][0], sorted_list_of_starting_state[1][0]
      return first_elem if first_elem != (self.env.observation_space.high[1] + 1, self.env.observation_space.high[0] + 1) else sec_elem

  def get_specific_reward(self, state, default_reward):
    """ this function is used to try custon init values for rewards """
    if state == (24, 24):
      return default_reward if self.init_final_state_reward == 'default' else self.init_final_state_reward
    
    else:
      if state == self.positive_state:
        return self.positive_reward
      elif state == self.negative_state:
        return self.negative_reward
      else:
        return default_reward if self.init_maze_reward == 'default' else self.init_maze_reward
  
  def generate_episode_from_Q(self, i_episode):
      """ generates an episode from following the epsilon-greedy policy """
      episode = []
      self.env.reset()

      ''' exploration using hot start state  '''
      if self.use_hot_start:
        self.env.state = self.get_starting_state()
      
      if not isinstance(self.env.state, tuple):
          self.env.state = tuple(self.env.state)

      state = self.env.state
      self.starting_states[self.env.state] += 1

      if state not in self.Q:
          action = self.env.action_space.sample()
      else:
        action = self.generate_action_from_state(state)

      num_of_steps, tot_reward_of_episode = 0, 0
      while True:
        if not isinstance(state, tuple):
          state = tuple(state)

        next_state, reward, done, info = self.env.step(self.nTa[action][1])
        next_state = tuple(next_state)
        reward = self.get_specific_reward(next_state, reward)
        next_action = self.generate_action_from_state(next_state)

        ''' keeping track of visited states for later uses of starting state '''
        
        self.starting_states[state] += 1
        self.starting_states[next_state] += 1

        self.update_Q(state, next_state, reward, action, next_action)

        state = next_state
        action = next_action
        num_of_steps += 1
        tot_reward_of_episode += reward

        if done:
          print("\rEpisode {}/{}, {} iterations.".format(i_episode, self.n_episodes, num_of_steps), end="")
          sys.stdout.flush()
          self.df.loc[len(self.df)] = num_of_steps, tot_reward_of_episode
          break

  def generate_action_from_state(self, state):
    if self.use_greedy_epsilon:
      policy_recommended_action = self.epsilon_greedy_action(self.Q[state])
    
    else:
      policy_recommended_action = np.argmax(self.Q[state])

    ''' using stochastic p to generate an action '''
    probs = np.where(self.actions == policy_recommended_action, self.p, (1 - self.p) / 3)
    action = np.random.choice(self.actions, p=probs)
    
    return action


  def epsilon_greedy_action(self, Qs):
      """ 
      obtains the action probabilities corresponding to epsilon-greedy policy.
      we will use random tie breaker in case of same probabilities
      """
      
      indices_of_max_prob = list(np.where(Qs == np.max(Qs))[0])
      selected_action = random.randrange(0, len(indices_of_max_prob))
      best_a = indices_of_max_prob[selected_action]

      greedy_policy = np.ones(self.nA) * self.epsilon / self.nA
      greedy_policy[best_a] = 1 - self.epsilon + (self.epsilon / self.nA)

      if random.random() < self.epsilon:
        non_best_actions = [i for i in self.actions if i != best_a]
        selected_non_best_action = random.randrange(0, len(non_best_actions))
        return non_best_actions[selected_non_best_action]

      else:
        return best_a

  def run_SARSA(self):
    self.nA = self.env.action_space.n
    self.actions = np.array([i for i in range(self.env.action_space.n)])
    rows = self.env.observation_space.high[0] + 1
    cols = self.env.observation_space.high[1] + 1
    self.starting_states = {(row, col): 0 for row in range(rows) for col in range(cols)}

    self.Q = defaultdict(lambda: np.zeros(self.nA))
    
    for i_episode in range(1, self.n_episodes + 1):     
        self.generate_episode_from_Q(i_episode)

        if i_episode == self.n_episodes // 2:
          self.middle_model = dict((k,np.argmax(v)) for k, v in self.Q.items())
        
        if i_episode in [self.n_episodes, self.n_episodes // 2]:
          self.episodes_to_starting_states_tracker[i_episode] = self.starting_states.copy()

    self.policy = dict((k,np.argmax(v)) for k, v in self.Q.items())

  def get_action_test_case(self, state):
    if state not in self.policy:
        action = self.env.action_space.sample()
    else:
      if random.random() < self.p:
        action = self.policy[state]
      else:
        action = random.randint(0, 3)
        while action == self.policy[state]:
          action = random.randint(0, 3)
          
    return action

  def measure_model_policy(self):
    rewards_sum, steps_sum = 0, 0
    self.use_greedy_epsilon=False
    for _ in range(100):
      done =False
      state = tuple(self.env.reset())
      
      while not done:
        action = self.generate_action_from_state(state)
        next_state, reward, done, info = self.env.step(self.nTa[action][1])
        next_state = tuple(next_state)
        next_action = self.generate_action_from_state(next_state)

        rewards_sum += reward
        steps_sum += 1

        self.update_Q(state, next_state, reward, action, next_action)
        state = tuple(next_state)
    
    return round(rewards_sum / 100, 4), round(steps_sum / 100, 4)

  def plot_learnt_path(model):
    fig, axs = plt.subplots(1, len(model.episodes_to_starting_states_tracker.items()), figsize=(20, 5))
    set_range = lambda new_val: (new_val - min_val) / (max_val - min_val)

    for i, (episode, mat) in enumerate(model.episodes_to_starting_states_tracker.items()):
      max_val = max([val for val in mat.values()])
      min_val = min([val for val in mat.values()])
      maze_array = np.zeros((25, 25))
      for row in range(25):
        for col in range(25):
          maze_array[row, col] = set_range(mat[(col, row)])

      axs[i].set_title(f"After {episode} episodes")
      sns.heatmap(maze_array, linewidth=3, ax=axs[i])

    fig.subplots_adjust(wspace=0.001)
    plt.show()

  def generate_video(self, filename, fps=10):
    self.env.reset()
    state = (0, 0)
    done = False  
    self.use_greedy_epsilon=False

    filename += '.mp4'
    with imageio.get_writer(filename, fps=fps) as video:
      video.append_data(self.env.render(mode='rgb_array'))
      while not done:        
        if random.random() < 0.9:
          action = self.policy[state]
        else:
          action = random.randint(0, 3)
          while action == self.policy[state]:
            action = random.randint(0, 3)

        next_state, reward, done, info = self.env.step(self.nTa[action][1])
        next_state = tuple(next_state)
        if random.random() < 0.9:
          next_action = self.policy[next_state]
        else:
          next_action = random.randint(0, 3)
          while next_action == self.policy[next_state]:
            next_action = random.randint(0, 3)

        self.update_Q(state, next_state, reward, action, next_action)
        state = tuple(next_state)
        video.append_data(self.env.render(mode='rgb_array'))

"""###Experiments"""

negative_states = [(3, 7), (5, 20), (13, 24)]
negative_rewards = [-25e-5, -30e-5, -20e-5]

positive_states = [(12, 7), (15, 16), (20, 21)]
positive_rewards = [-9e-4, -5e-4, -1e-4]

init_maze_rewards = ['default']
init_final_state_rewards = [0.1, 'default', 2]

combos = []
for negative_reward in negative_rewards:
  for negative_state in negative_states:
    for positive_reward in positive_rewards:
      for positive_state in positive_states:
        for init_maze_reward in init_maze_rewards:
          for init_final_state_reward in init_final_state_rewards:
            combos.append((negative_reward, negative_state, positive_reward, positive_state, init_maze_reward, init_final_state_reward))

best_hp = None
best_model = None
lowest_steps = 2000
df = pd.DataFrame(columns=['negative_reward', 'negative_state', 'positive_reward', 'positive_state', 'init_maze_reward', 'init_final_state_reward', 'avg_steps'])

for combo in combos:
  negative_reward, negative_state, positive_reward, positive_state, init_maze_reward, init_final_state_reward = combo

  display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()
  final_env = MazeEnvCast25x25()

  model = SARSA_custom_agent_trainer(env=final_env, gamma=0.99, alpha=0.1,
                                  epsilon=0.1, n_episodes=500,
                                  use_greedy_epsilon=False, use_hot_start=False,
                                  negative_state=negative_state, positive_state=positive_state,
                                  negative_reward=negative_reward, positive_reward=positive_reward,
                                  init_maze_reward=init_maze_reward, init_final_state_reward=init_final_state_reward)

  reward, steps = model.measure_model_policy()
  df.loc[len(df)] = negative_reward, negative_state, positive_reward, positive_state, init_maze_reward, init_final_state_reward, steps
  if steps < lowest_steps:
    best_hp = combo
    best_model = model
    lowest_steps = steps

"""Best parameters:"""

negative_reward, negative_state, positive_reward, positive_state = -0.00020, (3, 7), -0.0009, (12, 7)
init_maze_reward, init_final_state_reward = 'default', 1.0

display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()
final_env = MazeEnvCast25x25()
sarsa_custom_model = SARSA_custom_agent_trainer(env=final_env, gamma=0.99, alpha=0.1,
                                  epsilon=0.1, n_episodes=500,
                                  use_greedy_epsilon=False, use_hot_start=False,
                                  negative_state=negative_state, positive_state=positive_state,
                                  negative_reward=negative_reward, positive_reward=positive_reward,
                                  init_maze_reward=init_maze_reward, init_final_state_reward=init_final_state_reward)

"""###Results Graphs"""

sarsa_custom_model.plot_learnt_path()

plot_convergence(sarsa_custom_model.df)