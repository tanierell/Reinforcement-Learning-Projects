# -*- coding: utf-8 -*-
"""RL-Finals.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1stlnJyefaQcQUdRJtl5Wx6M7GwrijqaS

# Final Project - Reinforcements Learning 
Hello dear students,<br> this is the template notebook. Please click on the "File" tab and then on "Save a copy into drive".

---
<br>

### Name and ID:
Student 1: Etay Todress,  316517689
<br>
Student 2: Jonathan Erell,  207044447
<br><br>
<img src="https://play-lh.googleusercontent.com/e_oKlKPISbgdzut1H9opevS7-LTB8-8lsmpCdMkhlnqFenZhpjxbLmx7l158-xQQCIY">

### https://github.com/mpSchrader/gym-sokoban

#1 - Installations, imports, and display inits

## Installs
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !sudo apt-get update
# !sudo apt-get install -y xvfb ffmpeg freeglut3-dev
# !pip install 'imageio==2.4.0'
# !pip install gym
# !pip install pygame
# !apt-get install python-opengl -y
# !apt install xvfb -y
# !pip install pyvirtualdisplay
# !pip install piglet
# !pip install gym
# !apt-get install python-opengl -y
# !apt install xvfb -y
# !pip install gym_sokoban
# !git clone https://github.com/avivg7/sokoban-so.git
# !unzip /content/sokoban-so/Compress.zip

"""## Imports"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import gym
import cv2
import pickle
import pandas as pd
import seaborn as sns
from gym import logger as gymlogger
from gym.utils import seeding
from gym import error, spaces, utils
gymlogger.set_level(40) # error only
from soko_pap import *
import glob
import io
import gc
from copy import deepcopy
import base64
from keras.applications import VGG16
import os
import random
import matplotlib.pyplot as plt
# %matplotlib inline
import math
import glob
from collections import deque
from pyvirtualdisplay import Display
from IPython.display import HTML
from IPython import display as ipythondisplay
# import pygame
import pyvirtualdisplay
import imageio
import IPython
import time
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from keras.models import Model, load_model
from keras.layers import Input, Dense, Lambda, Add, Conv2D, Flatten
from keras.optimizers import Adam, RMSprop
from keras.losses import Huber
from keras import backend as K

"""## Display utils
The cell below contains the video display configuration. No need to make changes here.

{'easy': {1: [3,
              4,
              15,
              16,
              17,
              20,
              22,
              29,
              37,
              39,
              51,
              52,
              53,
              55,
              56,
              63,
              76,
              83,
              87,
              90,
              98],
          2: [8, 11, 12, 28, 35, 43, 46, 59, 62, 71, 72, 73, 80, 81, 84, 95]},
 'hard': {4: [6, 13, 18, 26, 33, 34, 38, 41, 42, 50, 68, 69, 78],
          5: [2, 5, 32, 49, 65, 67, 86],
          6: [24, 48]},
 'medium': {3: [0,
                1,
                7,
                9,
                10,
                14,
                19,
                21,
                23,
                25,
                27,
                30,
                31,
                36,
                40,
                44,
                45,
                47,
                54,
                57,
                58,
                60,
                61,
                64,
                66,
                70,
                74,
                75,
                77,
                79,
                82,
                85,
                88,
                89,
                91,
                92,
                93,
                94,
                96,
                97,
                99]}}
"""

def embed_mp4(filename):
  """Embeds an mp4 file in the notebook."""
  video = open(filename,'rb').read()
  b64 = base64.b64encode(video)
  tag = '''
  <video width="640" height="480" controls>
    <source src="data:video/mp4;base64,{0}" type="video/mp4">
  Your browser does not support the video tag.
  </video>'''.format(b64.decode())

  return IPython.display.HTML(tag)
display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()

"""Function - given an environment it will print us the details about observation, actions, agent's position and boxes locations"""

def print_env_det(env):
  print(f'Observation space: {env.observation_space}'
      f'\nAction space: {env.action_space}'
      f'\nPlayer position:{env.player_position}'
      f'\nBox location: {find_box_location(env)}')

"""Function - given an environment it will return the indexes position and boxes locations"""

def find_box_location(env):
    idx = np.argmax(env.room_state == 4)
    if env.room_state.flat[idx] == 4:
        return np.unravel_index(idx, env.room_state.shape)
    return None

#=============== DO NOT DELETE ===============
random.seed(2)
sok = PushAndPullSokobanEnv(dim_room=(7, 7),num_boxes=1 ,max_steps=200)
plt.imshow(sok.render('rgb_array'))
# ============================================

"""#2 - Helper Functions

###Experience Replay Implementation
"""

class PrioritizedReplayBuffer:
    def __init__(self, capacity, alpha=0.6, beta=0.4):
        self.capacity = capacity
        self.buffer = deque(maxlen=capacity)
        self.priorities = deque(maxlen=capacity)
        self.alpha = alpha
        self.beta = beta
        self.size = 0
    
    def add(self, state, action, reward, next_state, done):
        experience = (state, action, reward, next_state, done)
        priority = 1.0 if not self.buffer else max(self.priorities)
        self.buffer.append(experience)
        self.priorities.append(priority)
        self.size = len(self.priorities)
    
    def sample(self, batch_size):
        priorities = np.array(self.priorities, dtype=np.float32)
        probs = priorities ** self.alpha / np.sum(priorities ** self.alpha)
        indices = np.random.choice(self.size, batch_size, p=probs)
        weights = (self.size * probs[indices]) ** (-self.beta)
        weights /= np.max(weights)
        samples = [self.buffer[i] for i in indices]
        return map(np.array, zip(*samples)), indices, weights
    
    def update_priorities(self, indices, td_errors):
      for i, e in zip(indices, td_errors):
          self.priorities[i] = np.abs(e) + 1e-6

"""##Preprocess Input's Images

Setting an image grey scale from RGB, and crop it's not needed frames
"""

def preprocess_state(state_image_batch, x=17, y=17, size=78):
    crop_coords = (x, y, x + size, y + size)
    if len(state_image_batch.shape) == 3: 
      ''' single image case '''
      r, g, b = state_image_batch[:,:,0], state_image_batch[:,:,1], state_image_batch[:,:,2]
      gray_images = 0.2989 * r + 0.5870 * g + 0.1140 * b
      cropped_images = gray_images[crop_coords[1]:crop_coords[3], crop_coords[0]:crop_coords[2]]

    else:
      r, g, b = state_image_batch[:,:,:,0], state_image_batch[:,:,:,1], state_image_batch[:,:,:,2]
      gray_images = 0.2989 * r + 0.5870 * g + 0.1140 * b
      cropped_images = gray_images[:, crop_coords[1]:crop_coords[3], crop_coords[0]:crop_coords[2]]

    return cropped_images

"""##Get Objects Locations

- find_objects_locations function receives an environment, and returns the location of the alien, the box, target and whether the env is solved

- get_state_to_target_distance function receives an environment, uses a* algorithm for shortest path and returns the sortest path from the alien to the box, and from the box to the target
"""

def find_objects_locations(env):
    alian = np.unravel_index(np.argmax(env.room_state == 5), env.room_state.shape)
    box = np.unravel_index(np.argmax(env.room_state == 4), env.room_state.shape)
    target_ = np.argmax(env.room_state == 2)
    if target_ == 0:
      target = alian

    else:
      target = np.unravel_index(target_, env.room_state.shape)
      
    done = np.count_nonzero(env.room_state == 3) > 0

    return alian, box, target, done

def get_state_to_target_distance(sok):
  alian_loc, box_loc, target_loc, is_done = find_objects_locations(sok)
  if is_done:
    return 0, 0

  maze = sok.room_state.copy()
  maze = np.where(maze == 0, 1, 0)

  box_target_path = search(maze, 1, box_loc, target_loc)
  box_target_distance = np.max(np.array(box_target_path))
  alian_box_path = search(maze, 1, alian_loc, box_loc)
  alian_box_distance = np.max(np.array(alian_box_path)) - 1
  return box_target_distance, alian_box_distance

"""##Step in Environment

this function is used to move within the environment, and dealing with the fixed "0" action which has bug. in addition it reformats the reward if needed
"""

def step_and_get_reward(sok, state, action, reward_shaping):
    if reward_shaping:
          box_target_distance_before, alian_box_distance_before = get_state_to_target_distance(sok)
          score_before = box_target_distance_before + alian_box_distance_before

    if action == 0:
      state_next, reward, done = state, -0.1, False
    else:
      state_next, reward, done, _ = sok.step(action)
      state_next = np.array(state_next)

    if reward_shaping:
      box_target_distance_after, alian_box_distance_after = get_state_to_target_distance(sok)
      score_after = box_target_distance_after + alian_box_distance_after
      reward += score_before - score_after

    return sok, state_next, reward, done

"""##Check Convergence or Divergence"""

def check_if_stop_run(model, episode_count):
    if model.model_name in ['DQN', 'D2QN', 'D3QN']:
      if model.epsilon == 0:
        saved_name = {
            model.PER and model.reward_shaping: f'{model.model_name}_PER_RS_',
            model.PER and not model.reward_shaping: f'{model.model_name}_PER_',
            not model.PER and model.reward_shaping: f'{model.model_name}_RS_',
            not model.PER and not model.reward_shaping: f'{model.model_name}_',
        }.get(True)
        
        np.save(saved_name + 'run_reward', model.running_reward)
        np.save(saved_name + 'loss', model.loss_history)
        np.save(saved_name + 'episode_reward', model.episodic_reward_history)
        model.dnn_model.save_weights(f"{model.model_name}.h5")
        model.is_converged = True

      if model.epsilon <= 0.4 and model.total_dones <= 3:
        model.is_diverged = True    
    
    else:
      ''' model name is either PPO or A2C '''
      if model.running_reward[-1] >= 10:
        model.is_converged = True

      if episode_count == 500:
        model.is_diverged = True
        np.save('PPO_run_reward', model.running_reward)
        np.save('PPO_episode_reward', model.episodic_reward_history)
        np.save('PPO_critic_loss', model.critic_loss.losses)
        np.save('PPO_actor_loss', model.actor_loss.losses)

    return model.is_converged, model.is_diverged

"""##DNN Default Body Architecture

each model is defined as baseline with the following structure:
- Input layer
- Conv layer (32, 8, 4)
- Conv layer (64, 4, 2)
- Conv layer (64, 3, 1)
- Flatten layer
- FC layer (512)
"""

def build_default_model():  
  model = keras.Sequential()
  kernel_initializer = tf.keras.initializers.GlorotNormal()
  input_layer = Input(shape=(78, 78, 1))
  first_conv = layers.Conv2D(32, 8, strides=4, activation="relu", kernel_initializer=kernel_initializer)(input_layer)
  sec_conv = layers.Conv2D(64, 4, strides=2, activation="relu", kernel_initializer=kernel_initializer)(first_conv)
  third_conv = layers.Conv2D(64, 3, strides=1, activation="relu", kernel_initializer=kernel_initializer)(sec_conv)
  flatten_common = layers.Flatten()(third_conv)
  dense_common = Dense(512, activation="relu", kernel_initializer=kernel_initializer)(flatten_common)

  return dense_common, input_layer

"""##Convertion to TF tensor"""

def tensor_conversion(*args):
  return tuple([tf.convert_to_tensor(arg, dtype=tf.float32) for arg in args])

"""##A* Function"""

class Node:
    def __init__(self, parent=None, position=None):
        self.parent = parent
        self.position = position

        self.g = 0
        self.h = 0
        self.f = 0
    def __eq__(self, other):
        return self.position == other.position

def return_path(current_node,maze):
    path = []
    no_rows, no_columns = np.shape(maze)
    result = [[-1 for i in range(no_columns)] for j in range(no_rows)]
    current = current_node
    while current is not None:
        path.append(current.position)
        current = current.parent
    path = path[::-1]
    start_value = 0
    for i in range(len(path)):
        result[path[i][0]][path[i][1]] = start_value
        start_value += 1
    return result


def search(maze, cost, start, end):
    start_node = Node(None, tuple(start))
    start_node.g = start_node.h = start_node.f = 0
    end_node = Node(None, tuple(end))
    end_node.g = end_node.h = end_node.f = 0

    yet_to_visit_list = []  
    visited_list = [] 
    yet_to_visit_list.append(start_node)
    outer_iterations = 0
    max_iterations = (len(maze) // 2) ** 10
    move  =  [[-1, 0 ], # go up
              [ 0, -1], # go left
              [ 1, 0 ], # go down
              [ 0, 1 ]] # go right

    no_rows, no_columns = np.shape(maze)
    
    while len(yet_to_visit_list) > 0:
        outer_iterations += 1    
        current_node = yet_to_visit_list[0]
        current_index = 0
        for index, item in enumerate(yet_to_visit_list):
            if item.f < current_node.f:
                current_node = item
                current_index = index

        if outer_iterations > max_iterations:
            print ("giving up on pathfinding too many iterations")
            return return_path(current_node,maze)

        yet_to_visit_list.pop(current_index)
        visited_list.append(current_node)
        if current_node == end_node:
            return return_path(current_node,maze)

        children = []

        for new_position in move: 
            node_position = (current_node.position[0] + new_position[0], current_node.position[1] + new_position[1])
            if (node_position[0] > (no_rows - 1) or 
                node_position[0] < 0 or 
                node_position[1] > (no_columns -1) or 
                node_position[1] < 0):
                continue

            if maze[node_position[0]][node_position[1]] != 0:
                continue

            new_node = Node(current_node, node_position)
            children.append(new_node)

        for child in children:
            
            if len([visited_child for visited_child in visited_list if visited_child == child]) > 0:
                continue
            child.g = current_node.g + cost
            child.h = (((child.position[0] - end_node.position[0]) ** 2) + 
                       ((child.position[1] - end_node.position[1]) ** 2)) 

            child.f = child.g + child.h
            if len([i for i in yet_to_visit_list if child == i and child.g > i.g]) > 0:
                continue

            yet_to_visit_list.append(child)

"""##DQN Class

**PARAMETERS**
- model_name: The name of the model to be used in training and testing.
- gamma: The discount factor used in the calculation of future rewards. Default is 0.95.
- batch_size: The number of samples used in each training batch. Default is 32.
- max_steps_per_episode: The maximum number of steps allowed in each episode. Default is 120.
- replay_buffer_capacity: The maximum capacity of the replay buffer. Default is 1000.
- n_episodes: The number of episodes to be run during training. Default is 1000.
- TAU: The parameter used in the soft update of the target network. Default is 0.08.
- max_epsilon: The maximum value of epsilon used in the exploration-exploitation trade-off. Default is 1.
- min_epsilon: The minimum value of epsilon used in the exploration-exploitation trade-off. Default is 0.01.
- epsilon_decay: The rate of decay for epsilon during training. Default is 8e-5.
- use_PER: A Boolean flag indicating whether or not to use prioritized experience replay. Default is True.
- reward_shaping: A Boolean flag indicating whether or not to use reward shaping. Default is True.

**METHODS**
- build_model: constructs the DNN model with the specified architecture
- get_state_to_target_distance: computes the distance from the agent to the target object in the game
- set_epsilon_greedy: computes the epsilon value for the epsilon-greedy exploration strategy
- preprocess_state: preprocesses the input state images for input to the DNN model
- predict_actions: computes the Q-values for a batch of input states
- get_action_from_state: selects an action from the given state using epsilon-greedy exploration
- update_network: updates the DNN model by training on a batch of experiences
- update_target_model: updates the target DNN model by copying the weights of the DNN model
- add_batch_to_queue: adds a batch of experiences to the replay buffer
- fit_on_batch: samples a batch of experiences from the replay buffer and trains the DNN model on them
- train: trains the DQN agent on a Sokoban game environment
- check_done_and_monitor_run: checks if the episode is done and prints the current epsilon value and episode length
- save_model: saves the DNN model and target DNN model to disk
- load_model: loads the DNN model or target DNN model from disk
"""

class DQN:
  def __init__(self, model_name, gamma=0.95, batch_size=32, max_steps_per_episode=120,
               replay_buffer_capacity=1000, TAU=0.08, max_epsilon=1, 
               min_epsilon=0.01, epsilon_decay=8e-5, seeds_available=None,
               use_PER=False, reward_shaping=False, envs_seeds=None,
               specific_seed=2, use_pretrained=False, pretrained_path=None, max_episodes=100):
    

    self.gamma, self.batch_size, self.model_name = gamma, batch_size, model_name
    self.max_steps_per_episode, self.is_converged, self.is_diverged = max_steps_per_episode, False, False
    self.total_steps_counter, self.epsilon_extra_factor = 0, 0
    self.TAU, self.epsilon_decay = TAU, epsilon_decay
    self.epsilon, self.max_episodes = 1, max_episodes
    self.optimizer, self.loss_function = Adam(learning_rate=0.0001), Huber()
    self.loss_history, self.episodic_reward_history, self.running_reward = [], [], []
    self.reward_shaping, self.sok = reward_shaping, None
    self.consecutive_dones, self.total_dones = 0, 0
    self.set_init_parameters(use_pretrained, use_PER, specific_seed, envs_seeds,
                             replay_buffer_capacity, pretrained_path, seeds_available)
    self.middle_model = None
    
    random.seed(specific_seed)
    self.origin_env = PushAndPullSokobanEnv(dim_room=(7, 7),num_boxes=1 ,max_steps=self.max_steps_per_episode)
    random.seed()

  
  def set_init_parameters(self, use_pretrained, use_PER, specific_seed, envs_seeds, 
                          replay_buffer_capacity, pretrained_path, seeds_available):
    if use_pretrained:
      self.dnn_model, self.target_dnn_model = keras.models.load_model(pretrained_path), keras.models.load_model(pretrained_path)
 
    else:
      self.dnn_model, self.target_dnn_model = self.build_model(), self.build_model()

    
    self.use_PER = use_PER
    if self.use_PER:
      self.PER = PrioritizedReplayBuffer(capacity=replay_buffer_capacity)
    else:
      self.PER = False
      self.non_PER_queue = []
    
    if specific_seed is None:
      self.seeds_memory = {i: self.max_steps_per_episode for i in seeds_available}
    else:
      self.seeds_memory = {specific_seed: self.max_steps_per_episode}


  def build_model(self):
    dense_common, input_layer = build_default_model()
    kernel_initializer = tf.keras.initializers.GlorotNormal()
    dense_sec = Dense(256, activation="relu", kernel_initializer=kernel_initializer)(dense_common)

    if self.model_name == 'D3QN':
      ''' split to  state'''
      dense_critic = Dense(128, activation="relu", kernel_initializer=kernel_initializer)(dense_sec)
      dense_critic = Dense(64, activation="relu", kernel_initializer=kernel_initializer)(dense_critic)
      state_value = Dense(1, kernel_initializer='he_uniform')(dense_critic)
      state_value = Lambda(lambda s: K.expand_dims(s[:, 0], -1), output_shape=(13,))(state_value)

      ''' split to action '''
      dense_actor = Dense(128, activation="relu", kernel_initializer=kernel_initializer)(dense_sec)
      dense_actor = Dense(64, activation="relu", kernel_initializer=kernel_initializer)(dense_actor)
      action_advantage = Dense(13, kernel_initializer='he_uniform')(dense_actor)
      action_advantage = Lambda(lambda a: a[:, :] - K.mean(a[:, :], keepdims=True), output_shape=(13,))(action_advantage)
      model_output_layer = Add()([state_value, action_advantage])
      
    else:
      ''' model_name is either D2QN / DQN '''
      model_output_layer = Dense(13, activation='linear')(dense_common)

    model = Model(inputs=input_layer, outputs=model_output_layer)
    model.compile(loss=self.loss_function, optimizer=self.optimizer, metrics=["accuracy"])
    return model

  
  def predict_actions(self, states_batch, model):
    phi_state = preprocess_state(states_batch)
    state_tensor = tf.expand_dims(tf.convert_to_tensor(phi_state), 0)
    return model.predict(state_tensor, verbose=0)


  def get_action_from_state(self, states_batch, model, epsilon=None):
    greedy_prob = self.epsilon if epsilon is None else epsilon
    action = random.randint(0,12) if random.random() < greedy_prob else \
              tf.argmax(self.predict_actions(states_batch, model)[0]).numpy()
    return action


  def update_network(self, state_sample, action_sample, rewards_sample, 
                     state_next_sample, done_sample, weights):

      action_sample, rewards_sample, is_not_done, weights = \
      tensor_conversion(action_sample, rewards_sample, done_sample, weights)
      action_sample = tf.cast(action_sample, dtype=tf.int32)
      phi_states = preprocess_state(state_sample)
      phi_next_states = preprocess_state(state_next_sample)
      target_prediction_next_states = self.target_dnn_model.predict(phi_next_states, verbose=0)

      if self.model_name == 'DQN':
        targets = rewards_sample +  self.gamma * (1 - is_not_done) * tf.reduce_max(target_prediction_next_states, axis=1)

      else:
        ''' model_name is either D2QN / D3QN '''
        next_next_Q_values = self.dnn_model.predict(phi_next_states, verbose=0)
        next_next_argmax_actions = tf.cast(tf.argmax(next_next_Q_values, axis=1), tf.int32) # returns a'_max

        argmax_indices = tf.stack((tf.range(tf.shape(next_next_argmax_actions)[0]), next_next_argmax_actions), axis=1)
        y_j = tf.gather_nd(target_prediction_next_states, argmax_indices)
        targets = rewards_sample + self.gamma * (1 - is_not_done) * y_j

      with tf.GradientTape() as tape:
        masks = tf.one_hot(action_sample, 13)
        naive_predictions = self.dnn_model(phi_states)
        predictions = tf.reduce_sum(tf.multiply(naive_predictions, masks), axis=1)
        loss = tf.reduce_mean(weights * self.loss_function(targets, predictions))

      self.loss_history.append(loss)
      td_error = targets - predictions
      gradients = tape.gradient(loss, self.dnn_model.trainable_variables)  
      self.optimizer.apply_gradients(zip(gradients, self.dnn_model.trainable_variables))
      gc.collect()
      K.clear_session()
      return td_error


  def update_target_model(self):
    for t, e in zip(self.target_dnn_model.trainable_variables, self.dnn_model.trainable_variables):
      t.assign(t * (1 - self.TAU) + e * self.TAU)


  def add_new_experience(self, state, action, reward, state_next, done):
    if self.use_PER:
      self.PER.add(state, action, reward, state_next, done)
      return self.PER.size
    
    else:
      self.non_PER_queue.append((state, action, reward, state_next, done))
      return len(self.non_PER_queue)


  def fit_on_batch(self):
    if self.use_PER:
      (states, actions, rewards, next_states, dones), indices, weights = self.PER.sample(self.batch_size)
      td_errors = self.update_network(states, actions, rewards, next_states, dones, weights)
      self.PER.update_priorities(indices, td_errors)
      
    else:
      ''' non PER '''
      indices = np.random.choice(range(len(self.non_PER_queue)), size=self.batch_size)
      weights = np.ones(shape=(self.batch_size))
      samples = [self.non_PER_queue[i] for i in indices]
      (states, actions, rewards, next_states, dones), weights = map(np.array, zip(*samples)), weights
      td_errors = self.update_network(states, actions, rewards, next_states, dones, weights)

    self.update_target_model()

  def get_seeded_environment(self):
    seeds_values = np.array(list(self.seeds_memory.values())) ** 2
    seeds_probs = (seeds_values / np.sum(seeds_values))
    chosen_index = np.random.choice(list(self.seeds_memory.keys()), size=1, p=seeds_probs)[0]
    seed = chosen_index

    random.seed(seed)
    self.origin_env = PushAndPullSokobanEnv(dim_room=(7, 7),num_boxes=1 ,max_steps=self.max_steps_per_episode)
    random.seed()
    self.sok = deepcopy(self.origin_env)
    return seed


  def set_epsilon_greedy(self):
    temp_epsilon = 0.01 + 0.99 * \
              math.exp(-self.epsilon_decay * self.total_steps_counter) - self.epsilon_extra_factor
   
    self.epsilon = max(temp_epsilon, 0)  



  def train(self):
    train_accumulative_reward, episode_count = 0, 1
    while not self.is_converged and not self.is_diverged and episode_count < self.max_episodes:
      seed = self.get_seeded_environment()
      seed_total_steps, episode_reward = 0, 0
      state = np.array(self.sok.render('rgb_array'))
      for env_steps in range(1, self.max_steps_per_episode):
        seed_total_steps += 1
        self.total_steps_counter += 1
        self.set_epsilon_greedy()
        action = self.get_action_from_state(state, self.dnn_model)
        self.sok, state_next, reward, done = step_and_get_reward(self.sok, state, action, self.reward_shaping)
        episode_reward += reward
        train_accumulative_reward = 0.95 * train_accumulative_reward + 0.05 * reward
        self.running_reward.append(train_accumulative_reward)
        queue_size = self.add_new_experience(state, action, reward, state_next, done)
        state = state_next

        if queue_size >= self.batch_size:
          self.fit_on_batch()

        if done:
          self.total_dones += 1
          print(f"- - - - done after {seed_total_steps} steps")
          break

      self.episodic_reward_history.append(episode_reward)
      self.seeds_memory[seed] = seed_total_steps
      self.set_consecutive_dones_value(done)
      self.save_model(episode_count)
      self.monitor_run(episode_count, seed)
      episode_count += 1
      

  def monitor_run(self, episode_count, seed):
    print(f"Episode {episode_count}, total steps {self.total_steps_counter}, seed: {seed}, epsilon {round(self.epsilon, 3)}, running reward: {round(self.running_reward[-1], 2)}")
    print("- - - " * 5)

    check_if_stop_run(self, episode_count)


  def set_consecutive_dones_value(self, done):
    if done:
      self.consecutive_dones += 1
    else:
      self.consecutive_dones = 0

    if self.consecutive_dones % 5 == 0 and self.consecutive_dones > 0:
      self.epsilon_extra_factor += 0.1


  def save_model(self, episode_count):
    if self.model_name != "DQN":
      ''' model name is either D2QN / D3QN '''
      self.target_dnn_model.save(f'{self.model_name}_target.h5')

    self.dnn_model.save(f'{self.model_name}.h5')
    if episode_count == self.max_episodes // 2:
      self.dnn_model.save(f'{self.model_name}_middle.h5')

"""##PPO Class"""

class LossHistory(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.losses = []

    def on_batch_end(self, batch, logs={}):
        self.losses.append(logs.get('loss'))


class PPO:
  def __init__(self, gamma=0.99, critic_loss_function=keras.losses.Huber(),
               lr=7e-4, rho=0.99, eps=1e-5, nA=13, max_steps_per_episode=120,
               reward_shaping=True, specific_seed=2, model_name='PPO'):
    
    random.seed(specific_seed)
    self.origin_env = PushAndPullSokobanEnv(dim_room=(7, 7),num_boxes=1 ,max_steps=max_steps_per_episode)
    random.seed()   
    self.specific_seed, self.model_name = specific_seed, model_name
    self.max_steps_per_episode = max_steps_per_episode
    self.critic_loss_function = critic_loss_function
    self.actor_optimizer = keras.optimizers.RMSprop(learning_rate=lr, epsilon=eps, rho=rho, clipnorm=0.5)
    self.critic_optimizer = keras.optimizers.RMSprop(learning_rate=lr, epsilon=eps, rho=rho, clipnorm=0.5)
    self.nA, self.gamma = nA, gamma
    self.running_reward, self.episodic_reward_history = [], []
    self.total_steps_counter = 0
    self.actor_loss, self.critic_loss =  LossHistory(), LossHistory()
    self.states_history, self.actions_history, self.rewards_history, self.actor_predictions = [], [], [], []
    self.Actor, self.Critic = self.build_model()
    self.reward_shaping = reward_shaping
    self.is_converged, self.is_diverged = False, False


  def build_model(self):
    dense_common, input_layer = build_default_model()

    def ppo_loss(y_true, y_pred):
        advantages, prediction_picks, actions = y_true[:, :1], y_true[:, 1:1+self.nA], y_true[:, 1+self.nA:]
        LOSS_CLIPPING = 0.2
        ENTROPY_LOSS = 5e-3

        prob = y_pred * actions
        old_prob = actions * prediction_picks
        r = prob/(old_prob + 1e-10)
        p1 = r * advantages
        p2 = K.clip(r, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantages
        loss =  -K.mean(K.minimum(p1, p2) + ENTROPY_LOSS * -(prob * K.log(prob + 1e-10)))
        return loss

    ''' split to actor '''
    output_actor = Dense(self.nA, activation="softmax", kernel_initializer='he_uniform')(dense_common)
    Actor = Model(inputs=input_layer, outputs=output_actor)
    Actor.compile(loss=ppo_loss, optimizer=self.actor_optimizer)

    ''' split to critic '''
    output_critic = Dense(1, kernel_initializer='he_uniform')(dense_common)
    Critic = Model(inputs=input_layer, outputs=output_critic)
    Critic.compile(loss=self.critic_loss_function, optimizer=self.critic_optimizer)

    return Actor, Critic


  def phi_state(self, state):
    if isinstance(state, tf.Tensor):
      return state
    cropped_image = preprocess_state(state)
    phi_state = tf.convert_to_tensor(cropped_image)
    return tf.expand_dims(phi_state, 0)

  
  def discount_rewards(self, rewards):
      running_add = 0
      discounted_r = np.zeros_like(rewards)
      for i in reversed(range(0,len(rewards))):
        running_add = running_add * self.gamma + rewards[i]
        discounted_r[i] = running_add
      return discounted_r


  def get_trajectories(self, state, episode):
    episode_reward = 0
    for timestep in range(1, self.max_steps_per_episode):
      state = self.phi_state(state)
      actor_prediction = self.Actor.predict(state, verbose=0)
      action = np.random.choice(self.nA, p=np.squeeze(actor_prediction))
      self.sok, next_state, reward, done = step_and_get_reward(self.sok, state, action, self.reward_shaping)
      action_onehot = np.zeros([self.nA])
      action_onehot[action] = 1
      self.actions_history.append(action_onehot)
      self.actor_predictions.append(actor_prediction)
      self.rewards_history.append(reward)
      self.states_history.append(state)
      self.total_steps_counter += 1
      episode_reward += reward
      state = next_state
      if done:
        print(f"- - - - done after {timestep} steps")
        break

    return timestep, episode_reward, done


  def set_input_to_models(self):
    batch_states = tf.stack(self.states_history)
    batch_states_tf = tf.reshape(batch_states, shape=(batch_states.shape[0], 78, 78))
    batch_actions_tf = tf.stack(self.actions_history)
    batch_actions_tf = tf.cast(batch_actions_tf, dtype=tf.float32)
    actor_predictions = tf.stack(tf.convert_to_tensor(self.actor_predictions, dtype=tf.float32))
    batch_predictions_tf = tf.reshape(actor_predictions, shape=(actor_predictions.shape[0], self.nA))
    batch_discounted_r = self.discount_rewards(self.rewards_history).reshape(-1, 1)
    batch_discounted_r_tf = tf.convert_to_tensor(batch_discounted_r, dtype=tf.float32)
    return batch_states_tf, batch_actions_tf, batch_predictions_tf, batch_discounted_r

  
  def get_seeded_environment(self):
    seed = self.specific_seed
    self.sok = deepcopy(self.origin_env)
    return seed


  def train(self):
    train_accumuative_reward, episode_count, running_reward = 0, 0, 0
    while not self.is_converged and not self.is_diverged:
      seed = self.get_seeded_environment()
      state = np.array(self.sok.render('rgb_array'))
      timestep, episode_reward, done = self.get_trajectories(state, episode_count)

      running_reward = 0.05 * episode_reward + 0.95 * running_reward
      self.running_reward.append(running_reward)
      self.episodic_reward_history.append(episode_reward)
      batch_states_tf, batch_actions_tf, batch_predictions_tf, batch_discounted_r_tf = \
      self.set_input_to_models()
      batch_critic_values = self.Critic.predict(batch_states_tf, verbose=0)
      batch_advantages = batch_discounted_r_tf - batch_critic_values
      y_true = tf.concat([batch_advantages,
                          batch_predictions_tf,
                          batch_actions_tf], axis=1)
      self.Actor.fit(batch_states_tf, y_true, epochs=1, verbose=0, callbacks=[self.actor_loss])
      self.Critic.fit(batch_states_tf, batch_discounted_r_tf, epochs=1, verbose=0, callbacks=[self.critic_loss])
      gc.collect()
      K.clear_session()
      
      self.actions_history.clear()
      self.states_history.clear()
      self.rewards_history.clear()
      self.actor_predictions.clear()
      episode_count += 1
      self.check_done_and_monitor_run(episode_count, timestep, done, seed)


  def check_done_and_monitor_run(self, episode_count, timestep, done, seed):
    print(f"Finsihed episode {episode_count}, total steps {self.total_steps_counter}, seed {seed}, running reward: {round(self.running_reward[-1], 2)}")
    print("- - - " * 5)

    check_if_stop_run(self, episode_count)
             

  def save_model(self):
    self.Actor.save('Actor.h5')
    self.Critic.save('Critic.h5')
   

  def load_model(self, is_target=False, compile=True):
    Actor = keras.models.load_model('Actor.h5', compile=compile)
    Critic = keras.models.load_model('Critic.h5', compile=compile)
    return Actor, Critic

"""#3 - Ex 1 - Variations of DQN, D2QN, D3QN

##Without PER

###Simple DQN
"""

simple_DQN = DQN(model_name='DQN', specific_seed=2, use_PER=False, reward_shaping=False, use_pretrained=False)
simple_DQN.train()

import matplotlib.pyplot as plt
import numpy as np

x = np.linspace(0, 10, 200)
fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))

x = range(1, len(simple_DQN.episodic_reward_history[:]) + 1)
y = simple_DQN.episodic_reward_history[:]
axs[0].plot(x, y)
axs[0].set_title('episodic rewards')

x = range(1, len(simple_DQN.running_reward[:]) + 1)
y = simple_DQN.running_reward[:]
axs[1].plot(x, y)
axs[1].set_title('running rewards')

x = range(1, len(simple_DQN.loss_history[:]) + 1)
y = simple_DQN.loss_history[:]
axs[2].plot(x, y)
axs[2].set_title('loss history')
fig.suptitle('Simple DQN Graphs - without PER')
plt.show()

"""###D2QN"""

D2QN = DQN(model_name='D2QN', specific_seed=2, use_PER=False, reward_shaping=False, use_pretrained=False)
D2QN.train()

x = np.linspace(0, 10, 200)
fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))

x = range(1, len(D2QN.episodic_reward_history[:]) + 1)
y = D2QN.episodic_reward_history[:]
axs[0].plot(x, y)
axs[0].set_title('episodic rewards')

x = range(1, len(D2QN.running_reward[:]) + 1)
y = D2QN.running_reward[:]
axs[1].plot(x, y)
axs[1].set_title('running rewards')

x = range(1, len(D2QN.loss_history[:]) + 1)
y = D2QN.loss_history[:]
axs[2].plot(x, y)
axs[2].set_title('loss history')
fig.suptitle('D2QN Graphs')
plt.show()

"""###D3QN"""

D3QN = DQN(model_name='D3QN', specific_seed=2, use_PER=False, reward_shaping=False, use_pretrained=False)
D3QN.train()

x = np.linspace(0, 10, 200)
fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))

x = range(1, len(D3QN.episodic_reward_history[:]) + 1)
y = D3QN.episodic_reward_history[:]
axs[0].plot(x, y)
axs[0].set_title('episodic rewards')

x = range(1, len(D3QN.running_reward[:]) + 1)
y = D3QN.running_reward[:]
axs[1].plot(x, y)
axs[1].set_title('running rewards')

x = range(1, len(D3QN.loss_history[:]) + 1)
y = D3QN.loss_history[:]
axs[2].plot(x, y)
axs[2].set_title('loss history')
fig.suptitle('D3QN Graphs')
plt.show()

"""##Using PER

###Simple DQN
"""

simple_DQN = DQN(model_name='DQN', specific_seed=2, use_PER=True, reward_shaping=False, use_pretrained=False)
simple_DQN.train()

"""###D2QN"""

D2QN = DQN(model_name='D2QN', specific_seed=2, use_PER=True, reward_shaping=False, use_pretrained=False)
D2QN.train()

"""###D3QN"""

D3QN = DQN(model_name='D3QN', specific_seed=2, use_PER=True, reward_shaping=False)
D3QN.train()

"""##Visualizations Graphs - Hyper Parameters"""

custom_rewards_values = {'negative': {-0.1: -1, -1: -2, 1: 2, 10.9: 20},
                         'default': {-0.1: -0.1, -1: -1, 1: 1, 10.9: 10},
                         'positive': {-0.1: 0, -1: -0.5, 1: 1, 10.9: 15}}

gammas = [0.8, 0.9, 0.99]
use_epsilon_greedy = [True, False]
combinations = []
final_df_rewards = pd.DataFrame()
final_df_losses = pd.DataFrame()
for gamma in gammas:
  for use_greedy in use_epsilon_greedy:
    for custom_reward_type, custom_reward_dict in custom_rewards_values.items():
      combo = (gamma, use_greedy, custom_reward_type)
      combinations.append(combo)

for combo in combinations:
  gamma, use_greedy, custom_reward_type = combo
  model = DQN(model_name='D3QN', gamma=gamma, specific_seed=2, epsilon_greedy=use_greedy, custom_rewards_values=custom_reward_dict)
  model.train()
  name = f"{gamma}_{use_greedy}_" + custom_reward_type
  final_df_rewards[name] = model.episodic_reward_history
  final_df_losses[name] = model.loss_history

"""**The name of each graph label, corresponds to the following pattern: gamma_epsilonGreedyUsed_customInitRewards**

**First graph, represents a little sample of the hyper parameters - a function of the running reward in each episode**
"""

fig, ax = plt.subplots(figsize=(10, 6))
for col in final_df_rewards.iloc[:6]:
  sns.lineplot(data=final_df_rewards[col], ax=ax, linewidth=1.5)

ax.legend(loc='best', labels=final_df_rewards.columns)
plt.xlabel('Episodes')
plt.ylabel('Running Reward')
plt.show()

"""**Second graph, represents a little sample of the hyper parameters - a function of the average reward in each episode**"""

fig, ax = plt.subplots(figsize=(10, 6))
for col in final_df_losses.iloc[:6]:
  sns.lineplot(data=final_df_losses[col], ax=ax, linewidth=1.5)

ax.legend(loc='best', labels=final_df_losses.columns)
plt.xlabel('Episodes')
plt.ylabel('Running Reward')
plt.show()

"""##Videos

###Simple DQN - optimal solution (9 steps)
"""

random.seed(2)
sok = PushAndPullSokobanEnv(dim_room=(7, 7),num_boxes=1)
done = False
iter = 0
video_filename = 'imageio.mp4'
with imageio.get_writer(video_filename, fps=10) as video:
  state = sok.render('rgb_array')
  video.append_data(state)
  while not done:
    if done:
      break
    iter += 1
    action = simple_DQN.get_action_from_state(state, simple_DQN.dnn_model)
    state, reward, done, _ = sok.step(action)
    video.append_data(sok.render(mode='rgb_array'))
  
  print(f"Done in {iter} steps")
  
embed_mp4(video_filename)

"""###D2QN - optimal solution (9 steps)"""

random.seed(2)
sok = PushAndPullSokobanEnv(dim_room=(7, 7),num_boxes=1)
done = False
iter = 0
video_filename = 'imageio.mp4'
with imageio.get_writer(video_filename, fps=10) as video:
  state = sok.render('rgb_array')
  video.append_data(state)
  while not done:
    if done:
      break
    iter += 1
    action = D2QN.get_action_from_state(state, D2QN.dnn_model)
    state, reward, done, _ = sok.step(action)
    video.append_data(sok.render(mode='rgb_array'))
  
  print(f"Done in {iter} steps")
  
embed_mp4(video_filename)

"""###D3QN - optimal solution (9 steps)

**Middle Model D3QN**
"""

random.seed(2)
middle_D3QN = D3QN.middle_model
sok = PushAndPullSokobanEnv(dim_room=(7, 7),num_boxes=1)
done = False
iter = 0
video_filename = 'imageio.mp4'
with imageio.get_writer(video_filename, fps=10) as video:
  state = sok.render('rgb_array')
  video.append_data(state)
  while not done:
    if done:
      break
    iter += 1
    action = D3QN.get_action_from_state(state, middle_D3QN, epsilon=0.3)
    state, reward, done, _ = sok.step(action)
    video.append_data(sok.render(mode='rgb_array'))
  
  print(f"Done in {iter} steps")
  
embed_mp4(video_filename)

"""**Best Model**"""

random.seed(2)
sok = PushAndPullSokobanEnv(dim_room=(7, 7),num_boxes=1)
done = False
iter = 0
video_filename = 'imageio.mp4'
with imageio.get_writer(video_filename, fps=10) as video:
  state = sok.render('rgb_array')
  video.append_data(state)
  while not done:
    if done:
      break
    iter += 1
    action = D3QN.get_action_from_state(state, D3QN.dnn_model)
    state, reward, done, _ = sok.step(action)
    video.append_data(sok.render(mode='rgb_array'))
  
  print(f"Done in {iter} steps")
  
embed_mp4(video_filename)

"""#4 - Ex 2 - Experiments (Added PPO Algorithm)

##Experiment 1 - All the above models, using Reward Shaping

**The following experiment was ran over the environment of Ex1 - seed 2, for sanity check and overall directions**

###Using Reward Shaping Only, without PER

####Simple DQN
"""

simple_DQN = DQN(model_name='DQN', specific_seed=2, use_PER=False, reward_shaping=True, use_pretrained=False)
simple_DQN.train()

"""####D2QN"""

D2QN = DQN(model_name='D2QN', specific_seed=2, use_PER=False, reward_shaping=True, use_pretrained=False)
D2QN.train()

"""####D3QN"""

D2QN = DQN(model_name='D3QN', specific_seed=2, use_PER=False, reward_shaping=True, use_pretrained=False)
D2QN.train()

"""###Using both PER and Reward Shaping

####Simple DQN Run
"""

simple_DQN = DQN(model_name='DQN', specific_seed=2, use_PER=True, reward_shaping=True, use_pretrained=False)
simple_DQN.train()

"""####D2QN Run"""

D2QN = DQN(model_name='D2QN', specific_seed=2, use_PER=True, reward_shaping=True, use_pretrained=False)
D2QN.train()

"""####D3QN Run"""

D3QN = DQN(model_name='D3QN', specific_seed=2, use_PER=True, reward_shaping=True, use_pretrained=False)
D3QN.train()

"""####PPO Algorithm"""

PPO_model = PPO(max_steps_per_episode=120, reward_shaping=False)
PPO_model.train()

"""###Graphs and Plots - Loss, and Rewards (using both PER and Reward Shaping)

####Simple DQN
"""

import matplotlib.pyplot as plt
import numpy as np

x = np.linspace(0, 10, 200)
fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))

x = range(1, len(simple_DQN.episodic_reward_history[:]) + 1)
y = simple_DQN.episodic_reward_history[:]
axs[0].plot(x, y)
axs[0].set_title('episodic rewards')

x = range(1, len(simple_DQN.running_reward[:]) + 1)
y = simple_DQN.running_reward[:]
axs[1].plot(x, y)
axs[1].set_title('running rewards')

x = range(1, len(simple_DQN.loss_history[:]) + 1)
y = simple_DQN.loss_history[:]
axs[2].plot(x, y)
axs[2].set_title('loss history')
fig.suptitle('Simple DQN Graphs')
plt.show()

"""####D2QN"""

x = np.linspace(0, 10, 200)
fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))

x = range(1, len(D2QN.episodic_reward_history[:]) + 1)
y = D2QN.episodic_reward_history[:]
axs[0].plot(x, y)
axs[0].set_title('episodic rewards')

x = range(1, len(D2QN.running_reward[:]) + 1)
y = D2QN.running_reward[:]
axs[1].plot(x, y)
axs[1].set_title('running rewards')

x = range(1, len(D2QN.loss_history[:]) + 1)
y = D2QN.loss_history[:]
axs[2].plot(x, y)
axs[2].set_title('loss history')
fig.suptitle('D2QN Graphs')
plt.show()

"""####D3QN"""

x = np.linspace(0, 10, 200)
fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))

x = range(1, len(D3QN.episodic_reward_history[:]) + 1)
y = D3QN.episodic_reward_history[:]
axs[0].plot(x, y)
axs[0].set_title('episodic rewards')

x = range(1, len(D3QN.running_reward[:]) + 1)
y = D3QN.running_reward[:]
axs[1].plot(x, y)
axs[1].set_title('running rewards')

x = range(1, len(D3QN.loss_history[:]) + 1)
y = D3QN.loss_history[:]
axs[2].plot(x, y)
axs[2].set_title('loss history')
fig.suptitle('D3QN Graphs')
plt.show()

"""####All Together

**The Models which where check are simple DQN, D2QN, D3QN, PPO,**
**all have been tried with PER, and with reward shaping.**
**The legend names are provided as model_PER_RS (RS for reward shaping used)**
"""

with open('experiment_1_models_rewards.pkl', 'rb') as f:
  data = pickle.load(f)

for name, vals in data.items():
  sns.lineplot(data=vals, ax=ax, linewidth=1.5)

ax.legend(loc='best', labels=data.keys())
plt.xlabel('Episodes')
plt.ylabel('Running Reward')
plt.show()

"""###Best D3QN run - over random environment"""

seeds_available = random.sample(range(101), 50)
best_D3QN = DQN(model_name='D3QN', seeds_available=seeds_available)
best_D3QN.train()

import matplotlib.pyplot as plt
import numpy as np

x = np.linspace(0, 10, 200)
fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))

x = range(1, len(best_D3QN.episodic_reward_history[:]) + 1)
y = best_D3QN.episodic_reward_history[:]
axs[0].plot(x, y)
axs[0].set_title('episodic rewards')

x = range(1, len(best_D3QN.running_reward[:]) + 1)
y = best_D3QN.running_reward[:]
axs[1].plot(x, y)
axs[1].set_title('running rewards')

x = range(1, len(best_D3QN.loss_history[:]) + 1)
y = best_D3QN.loss_history[:]
axs[2].plot(x, y)
axs[2].set_title('loss history')
fig.suptitle('best D3QN Graphs')
plt.show()

"""##Experiment 2

**We will try using the best model so far which was the D3QN. For now, we will try to train on list of 5 environments**

**Training a D3QN model on multiple envs which are solvable with 1 step (pre checked), to use as a pretrain for later use**

###Train a pre-train D3QN
"""

seeds_available = [3, 4, 15, 16, 17, 20]
pretrain_D3QN = DQN(model_name='D3QN', reward_shaping=True, use_PER=True, seeds_available=seeds_available)
seeds_available = [3, 4, 15, 16, 17, 20]
pretrain_D3QN.train()

seeds_available = [0, 1, 2, 3, 4]

D3QN = DQN(model_name='D3QN', seeds_available=seeds_available, reward_shaping=True, use_PER=True, use_pretrained=True)
D3QN.train()

"""###Non convergence graphs"""

x = np.linspace(0, 10, 200)
fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))

x = range(1, len(D3QN.episodic_reward_history[:]) + 1)
y = D3QN.episodic_reward_history[:]
axs[0].plot(x, y)
axs[0].set_title('episodic rewards')

x = range(1, len(D3QN.running_reward[:]) + 1)
y = D3QN.running_reward[:]
axs[1].plot(x, y)
axs[1].set_title('running rewards')

x = range(1, len(D3QN.loss_history[:]) + 1)
y = D3QN.loss_history[:]
axs[2].plot(x, y)
axs[2].set_title('loss history')
fig.suptitle('D3QN Graphs')
plt.show()

"""##Experiment 3 - pre trained VGG"""

class VGG_DQN:
  def __init__(self, model_name, seeds_available, gamma=0.99, epsilon_greedy=True, custom_rewards_values=0, batch_size=32, max_steps_per_episode=120,
               replay_buffer_capacity=1000, TAU=0.08, max_epsilon=1, 
               min_epsilon=0.01, epsilon_decay=8e-6,
               use_PER=True, reward_shaping=True, envs_seeds=None,
               specific_seed=None, use_pretrained=False):
    

    self.gamma, self.batch_size, self.model_name = gamma, batch_size, model_name
    self.max_steps_per_episode, self.is_converged, self.is_diverged = max_steps_per_episode, False, False
    self.total_steps_counter, self.epsilon_extra_factor = 0, 0
    self.TAU, self.epsilon_decay = TAU, epsilon_decay
    self.epsilon_greedy = epsilon_greedy
    self.max_epsilon, self.min_epsilon, self.epsilon = max_epsilon, min_epsilon, 1
    self.optimizer, self.loss_function = Adam(learning_rate=0.0001), Huber()
    self.loss_history, self.episodic_reward_history, self.running_reward = [], [], []
    self.reward_shaping, self.sok = reward_shaping, None
    self.seeds_tiebreaker, self.consecutive_dones, self.total_dones = [], 0, 0
    self.set_init_parameters(use_pretrained, use_PER, specific_seed, seeds_available,
                             replay_buffer_capacity)

  
  def set_init_parameters(self, use_pretrained, use_PER, specific_seed, seeds_available, 
                          replay_buffer_capacity):
    if use_pretrained:
      self.load_model(compile=True)
 
    else:
      self.dnn_model, self.target_dnn_model = self.build_model(), self.build_model()
    
    self.use_PER = use_PER
    if self.use_PER:
      self.PER = PrioritizedReplayBuffer(capacity=replay_buffer_capacity)
    else:
      self.PER = False
      self.non_PER_queue = []
    
    if specific_seed is None:
      self.seeds_memory = {i: self.max_steps_per_episode for i in seeds_available}
    else:
      self.seeds_memory = {specific_seed: self.max_steps_per_episode}


  def build_model(self):
    vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(78, 78, 3))

    for layer in vgg_model.layers:
        layer.trainable = False

    kernel_initializer = tf.keras.initializers.GlorotNormal()
    x = layers.Flatten()(vgg_model.output)
    dense_first = layers.Dense(512, activation="relu", kernel_initializer=kernel_initializer)(x)

    state_value = Dense(1, kernel_initializer='he_uniform')(dense_first)
    state_value = Lambda(lambda s: K.expand_dims(s[:, 0], -1), output_shape=(13,))(state_value)

    ''' split to action '''
    action_advantage = Dense(13, kernel_initializer='he_uniform')(dense_first)
    action_advantage = Lambda(lambda a: a[:, :] - K.mean(a[:, :], keepdims=True), output_shape=(13,))(action_advantage)
    model_output_layer = Add()([state_value, action_advantage])

    model = Model(inputs=vgg_model.input, outputs=model_output_layer)
    model.compile(loss='mse', optimizer=self.optimizer, metrics=["accuracy"])
    return model

  def get_state_to_target_distance(self):
    alian_loc, box_loc, target_loc, is_done = find_objects_locations(self.sok)
    if is_done:
      return 0, 0

    maze = self.sok.room_state.copy()
    maze = np.where(maze == 0, 1, 0)

    box_target_path = search(maze, 1, box_loc, target_loc)
    box_target_distance = np.max(np.array(box_target_path))
    alian_box_path = search(maze, 1, alian_loc, box_loc)
    alian_box_distance = np.max(np.array(alian_box_path)) - 1
    return box_target_distance, alian_box_distance

  
  def preprocess_state(self, state_image_batch, x=17, y=17, size=78):
    crop_coords = (x, y, x + size, y + size)
    if len(state_image_batch.shape) == 3: 
      ''' single image case '''
      cropped_images = state_image_batch[crop_coords[1]:crop_coords[3], crop_coords[0]:crop_coords[2]]

    else:
      cropped_images = state_image_batch[:, crop_coords[1]:crop_coords[3], crop_coords[0]:crop_coords[2]]
    return cropped_images

  
  def predict_actions(self, states_batch, model):
    phi_state = self.preprocess_state(states_batch)
    state_tensor = tf.expand_dims(tf.convert_to_tensor(phi_state), 0)
    return model.predict(state_tensor, verbose=0)


  def get_action_from_state(self, states_batch, model, epsilon=None):
    if self.epsilon_greedy:
      greedy_prob = self.epsilon if epsilon is None else epsilon
      action = random.randint(0,12) if random.random() < greedy_prob else \
                tf.argmax(self.predict_actions(states_batch, model)[0]).numpy()

    else:
      action = tf.argmax(self.predict_actions(states_batch, model)[0]).numpy()
    
    return action


  def update_network(self, state_sample, action_sample, rewards_sample, 
                     state_next_sample, done_sample, weights):

      action_sample = tf.convert_to_tensor(action_sample)
      rewards_sample = tf.convert_to_tensor(rewards_sample, dtype=tf.float32)
      done_sample = tf.convert_to_tensor(done_sample, dtype=tf.float32)
      weights = tf.convert_to_tensor(weights, dtype=tf.float32)
      
      is_not_done = tf.convert_to_tensor(done_sample, dtype=tf.float32)
      phi_states = self.preprocess_state(state_sample)
      phi_next_states = self.preprocess_state(state_next_sample)
      target_prediction_next_states = self.target_dnn_model.predict(phi_next_states, verbose=0)

      if self.model_name == 'DQN':
        targets = rewards_sample +  self.gamma * (1 - is_not_done) * tf.reduce_max(target_prediction_next_states, axis=1)

      else:
        ''' model_name is either D2QN / D3QN '''
        next_next_Q_values = self.dnn_model.predict(phi_next_states, verbose=0)
        next_next_argmax_actions = tf.cast(tf.argmax(next_next_Q_values, axis=1), tf.int32) # returns a'_max

        argmax_indices = tf.stack((tf.range(tf.shape(next_next_argmax_actions)[0]), next_next_argmax_actions), axis=1)
        y_j = tf.gather_nd(target_prediction_next_states, argmax_indices)
        targets = rewards_sample + self.gamma * (1 - is_not_done) * y_j

      with tf.GradientTape() as tape:
        masks = tf.one_hot(action_sample, 13)
        naive_predictions = self.dnn_model(phi_states)
        predictions = tf.reduce_sum(tf.multiply(naive_predictions, masks), axis=1)
        loss = tf.reduce_mean(weights * self.loss_function(targets, predictions))

      td_error = targets - predictions
      gradients = tape.gradient(loss, self.dnn_model.trainable_variables)  
      self.optimizer.apply_gradients(zip(gradients, self.dnn_model.trainable_variables))
      return td_error, loss


  def update_target_model(self):
    for t, e in zip(self.target_dnn_model.trainable_variables, self.dnn_model.trainable_variables):
      t.assign(t * (1 - self.TAU) + e * self.TAU)


  def add_new_experience(self, state, action, reward, state_next, done):
    if self.use_PER:
      self.PER.add(state, action, reward, state_next, done)
      return self.PER.size
    
    else:
      self.non_PER_queue.append((state, action, reward, state_next, done))
      return len(self.non_PER_queue)


  def fit_on_batch(self):
    if self.use_PER:
      (states, actions, rewards, next_states, dones), indices, weights = self.PER.sample(self.batch_size)
      td_errors, loss = self.update_network(states, actions, rewards, next_states, dones, weights)
      self.PER.update_priorities(indices, td_errors)
      
    else:
      ''' non PER '''
      indices = np.random.choice(range(len(self.non_PER_queue)), size=self.batch_size)
      weights = np.ones(shape=(self.batch_size))
      samples = [self.non_PER_queue[i] for i in indices]
      (states, actions, rewards, next_states, dones), weights = map(np.array, zip(*samples)), weights
      td_errors, loss = self.update_network(states, actions, rewards, next_states, dones, weights)

    self.update_target_model()
    return loss

  def get_seeded_environment(self):
    my_list = list(self.seeds_memory.items())
    random.shuffle(my_list)
    self.seeds_memory = dict(my_list)
    seeds_values = np.array(list(self.seeds_memory.values())) ** 2
    seeds_probs = (seeds_values / np.sum(seeds_values))
    chosen_index = np.random.choice(list(self.seeds_memory.keys()), size=1, p=seeds_probs)[0]
    seed = chosen_index

    random.seed(seed)
    self.origin_env = PushAndPullSokobanEnv(dim_room=(7, 7),num_boxes=1 ,max_steps=self.max_steps_per_episode)
    random.seed()
    self.sok = deepcopy(self.origin_env)
    return seed


  def set_epsilon_greedy(self):
    temp_epsilon = self.min_epsilon + (self.max_epsilon - self.min_epsilon) * \
              math.exp(-self.epsilon_decay * self.total_steps_counter) - self.epsilon_extra_factor
   
    self.epsilon = max(temp_epsilon, 0)  


  def train(self):
    train_accumulative_reward, episode_count, episode_loss = 0, 1, 0
    while not self.is_converged and episode_count <= 400:
      seed = self.get_seeded_environment()
      seed_total_steps, episode_reward = 0, 0
      state = np.array(self.sok.render('rgb_array'))
      for env_steps in range(1, self.max_steps_per_episode):
        self.set_epsilon_greedy()
        action = self.get_action_from_state(state, self.dnn_model)
        if self.reward_shaping:
          box_target_distance_before, alian_box_distance_before = self.get_state_to_target_distance()
          score_before = box_target_distance_before + alian_box_distance_before

        self.total_steps_counter += 1
        seed_total_steps += 1
        if action == 0:
          state_next, reward, done = state, -0.1, False
        else:
          state_next, reward, done, _ = self.sok.step(action)
          state_next = np.array(state_next)

        if self.reward_shaping:
          box_target_distance_after, alian_box_distance_after = self.get_state_to_target_distance()
          score_after = box_target_distance_after + alian_box_distance_after
          reward += score_before - score_after

        episode_reward += reward
        train_accumulative_reward = 0.95 * train_accumulative_reward + 0.05 * reward
        self.running_reward.append(train_accumulative_reward)
        queue_size = self.add_new_experience(state, action, reward, state_next, done)
        state = state_next

        if queue_size >= self.batch_size:
          batch_loss = self.fit_on_batch()
          episode_loss += batch_loss.numpy()
          

        if done:
          self.total_dones += 1
          print(f"- - - - done after {seed_total_steps} steps")
          break

      self.episodic_reward_history.append(episode_reward)
      self.loss_history.append(episode_loss / env_steps)
      self.seeds_memory[seed] = seed_total_steps
      self.set_consecutive_dones_value(done)
      self.save_model()
      self.monitor_run(episode_count, seed)
      episode_count += 1


  def monitor_run(self, episode_count, seed):
    print(f"Episode {episode_count}, total steps {self.total_steps_counter}, epsilon {round(self.epsilon, 3)}, running reward: {round(self.running_reward[-1], 2)}, seeds memory {self.seeds_memory}")
    print("- - - " * 5)

    if self.running_reward[-1] > 5:
      print(f"****** CONVERGED *******")
      


  def set_consecutive_dones_value(self, done):
    if done:
      self.consecutive_dones += 1
    else:
      self.consecutive_dones = 0

    if self.consecutive_dones % 5 == 0 and self.consecutive_dones > 0:
      self.epsilon_extra_factor += 0.1


  def save_model(self):
    if self.model_name != "DQN":
      self.target_dnn_model.save(f'{self.model_name}_vgg_target.h5')
    
    self.dnn_model.save(f'{self.model_name}_vgg.h5')

vgg_model = VGG_DQN(model_name='D3QN', seeds_available=[3, 4, 15, 16, 17, 20, 22, 29, 37, 39, 51])
vgg_model.train()

"""####Non convergence graphs"""

x = np.linspace(0, 10, 200)
fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))

x = range(1, len(vgg_model.episodic_reward_history[:]) + 1)
y = vgg_model.episodic_reward_history[:]
axs[0].plot(x, y)
axs[0].set_title('episodic rewards')

x = range(1, len(vgg_model.running_reward[:]) + 1)
y = vgg_model.running_reward[:]
axs[1].plot(x, y)
axs[1].set_title('running rewards')

x = range(1, len(vgg_model.loss_history[:]) + 1)
y = vgg_model.loss_history[:]
axs[2].plot(x, y)
axs[2].set_title('loss history')
fig.suptitle('vgg_model Graphs')
plt.show()

"""#5 - Test Environment"""

def preprocess_state(state_image_batch, x=17, y=17, size=78):
    crop_coords = (x, y, x + size, y + size)
    ''' single image case '''
    r, g, b = state_image_batch[:,:,0], state_image_batch[:,:,1], state_image_batch[:,:,2]
    gray_images = 0.2989 * r + 0.5870 * g + 0.1140 * b
    cropped_images = gray_images[crop_coords[1]:crop_coords[3], crop_coords[0]:crop_coords[2]]

    return cropped_images

  
def predict_actions(states_batch, model):
  phi_state = preprocess_state(states_batch)
  state_tensor = tf.expand_dims(tf.convert_to_tensor(phi_state), 0)
  return model.predict(state_tensor, verbose=0)


def get_action_from_state(states_batch, model, epsilon=0):
  action = random.randint(0,12) if random.random() < epsilon else \
            tf.argmax(predict_actions(states_batch, model)[0]).numpy()
  return action

!gdown --id 1lDPNq2eRyRpPKO4eUohyw8krGcfNrAwq
!unzip /content/explainer_best_model.zip
tested_model = keras.models.load_model('optimal_D3QN.h5')

random.seed(2)
sok = PushAndPullSokobanEnv(dim_room=(7, 7),num_boxes=1)
random.seed()

done = False
iter = 0
video_filename = 'imageio.mp4'
with imageio.get_writer(video_filename, fps=10) as video:
  state = sok.render('rgb_array')
  video.append_data(state)
  while not done:
    if done:
      break
    iter += 1
    action = get_action_from_state(state, tested_model, epsilon=0)
    state, reward, done, _ = sok.step(action)
    video.append_data(sok.render(mode='rgb_array'))
  
  print(f"Done in {iter} steps")
  
embed_mp4(video_filename)